{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe9f2c7-8bd4-4efc-8180-34d7f7445493",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../utils\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clean_text, map_labels_to_binary, encode_categorical_features\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Starting Data Cleaning Process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment setup complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from preprocessing import clean_text, map_labels_to_binary, encode_categorical_features\n",
    "\n",
    "print(\"📊 Starting Data Cleaning Process...\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7f9b43-0d64-4c32-a437-b0fc9d57e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully!\n",
      "Training data: 10240 samples\n",
      "Test data: 1267 samples\n",
      "Validation data: 1284 samples\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    'id', 'label', 'statement', 'subject', 'speaker', \n",
    "    'job', 'state', 'party', 'barely_true', 'false', \n",
    "    'half_true', 'mostly_true', 'pants_on_fire', 'context'\n",
    "]\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train.tsv', sep='\\t', names=columns, encoding='utf-8')\n",
    "    test_df = pd.read_csv('../data/test.tsv', sep='\\t', names=columns, encoding='utf-8')\n",
    "    valid_df = pd.read_csv('../data/valid.tsv', sep='\\t', names=columns, encoding='utf-8')\n",
    "    \n",
    "    print(\"✅ Data loaded successfully!\")\n",
    "    print(f\"Training data: {len(train_df)} samples\")\n",
    "    print(f\"Test data: {len(test_df)} samples\")\n",
    "    print(f\"Validation data: {len(valid_df)} samples\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(\"❌ Error loading data files!\")\n",
    "    print(\"Please ensure train.tsv, test.tsv, and valid.tsv are in the data/ folder\")\n",
    "    print(\"Download from: https://github.com/thiagorainmaker77/liar_dataset\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e09e71-9348-460f-b1a2-92ae8dfaada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 DATA EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n🔍 Training Data Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n🔍 First 5 rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n🔍 Label Distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(\"\\n🔍 Missing Values:\")\n",
    "missing_vals = train_df.isnull().sum()\n",
    "print(missing_vals[missing_vals > 0])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_df['label'].value_counts().plot(kind='bar', rot=45)\n",
    "plt.title('Original Label Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "train_df['party'].value_counts().head(10).plot(kind='bar', rot=45)\n",
    "plt.title('Top 10 Political Parties')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60fdd65-92b1-4206-bede-1321528bbd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 CLEANING TEXT DATA...\n",
      "\n",
      "🔄 Processing Training dataset...\n",
      "  - Cleaning statement text...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_clean\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Process all datasets\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m train_clean \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m test_clean \u001b[38;5;241m=\u001b[39m process_dataset(test_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m valid_clean \u001b[38;5;241m=\u001b[39m process_dataset(valid_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(df, dataset_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Cleaning statement text...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_statement\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mclean_text\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Converting labels to binary...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(map_labels_to_binary)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧹 CLEANING TEXT DATA...\")\n",
    "\n",
    "def process_dataset(df, dataset_name):\n",
    "    \"\"\"Process a single dataset with cleaning and feature engineering\"\"\"\n",
    "    print(f\"\\n🔄 Processing {dataset_name} dataset...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "\n",
    "    print(\"  - Cleaning statement text...\")\n",
    "    df_clean['statement'] = df_clean['statement'].fillna('')\n",
    "    df_clean['clean_statement'] = df_clean['statement'].apply(clean_text)\n",
    "    \n",
    "    print(\"  - Converting labels to binary...\")\n",
    "    df_clean['label_binary'] = df_clean['label'].apply(map_labels_to_binary)\n",
    "    \n",
    "    print(\"  - Processing metadata...\")\n",
    "    df_clean['party'] = df_clean['party'].fillna('none')\n",
    "    df_clean['subject'] = df_clean['subject'].fillna('none')\n",
    "    df_clean['speaker'] = df_clean['speaker'].fillna('unknown')\n",
    "    df_clean['job'] = df_clean['job'].fillna('unknown')\n",
    "    df_clean['state'] = df_clean['state'].fillna('unknown')\n",
    "    \n",
    "    categorical_cols = ['party', 'subject', 'speaker', 'job', 'state']\n",
    "    df_clean = encode_categorical_features(df_clean, categorical_cols)\n",
    "    \n",
    "    df_clean['statement_length'] = df_clean['clean_statement'].str.len()\n",
    "    df_clean['word_count'] = df_clean['clean_statement'].str.split().str.len()\n",
    "\n",
    "    df_clean = df_clean[df_clean['clean_statement'].str.len() > 0]\n",
    "    \n",
    "    print(f\"  ✅ {dataset_name} processing complete!\")\n",
    "    print(f\"     Original size: {len(df)} -> Clean size: {len(df_clean)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Process all datasets\n",
    "train_clean = process_dataset(train_df, \"Training\")\n",
    "test_clean = process_dataset(test_df, \"Test\")\n",
    "valid_clean = process_dataset(valid_df, \"Validation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b353c9-060e-4d7d-934d-56d312c5c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 DATA QUALITY ANALYSIS\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ✅ No empty statements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m analyze_dataset(\u001b[43mtrain_clean\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m analyze_dataset(test_clean, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m analyze_dataset(valid_clean, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_clean' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n📈 DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_dataset(df, name):\n",
    "    \"\"\"Analyze cleaned dataset quality\"\"\"\n",
    "    print(f\"\\n📊 {name} Dataset Analysis:\")\n",
    "    print(f\"   Total samples: {len(df)}\")\n",
    "    print(f\"   Binary label distribution:\")\n",
    "    print(f\"     Real news (1): {sum(df['label_binary'] == 1)}\")\n",
    "    print(f\"     Fake news (0): {sum(df['label_binary'] == 0)}\")\n",
    "    print(f\"   Average statement length: {df['statement_length'].mean():.1f} chars\")\n",
    "    print(f\"   Average word count: {df['word_count'].mean():.1f} words\")\n",
    "    \n",
    "    empty_statements = sum(df['clean_statement'].str.len() == 0)\n",
    "    if empty_statements > 0:\n",
    "        print(f\"   ⚠  Empty statements found: {empty_statements}\")\n",
    "    else:\n",
    "        print(f\"   ✅ No empty statements\")\n",
    "\n",
    "analyze_dataset(train_clean, \"Training\")\n",
    "analyze_dataset(test_clean, \"Test\")\n",
    "analyze_dataset(valid_clean, \"Validation\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "train_clean['label_binary'].value_counts().plot(kind='bar', color=['red', 'green'])\n",
    "plt.title('Binary Labels\\n(0=Fake, 1=Real)')\n",
    "plt.xticks([0, 1], ['Fake', 'Real'], rotation=0)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(train_clean['statement_length'], bins=50, alpha=0.7, color='blue')\n",
    "plt.title('Statement Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(train_clean['word_count'], bins=50, alpha=0.7, color='green')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Words')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "train_clean['subject'].value_counts().head(10).plot(kind='barh')\n",
    "plt.title('Top 10 Subjects')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "party_counts = train_clean['party'].value_counts().head(8)\n",
    "plt.pie(party_counts.values, labels=party_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Political Party Distribution')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "party_label_crosstab = pd.crosstab(train_clean['party'], train_clean['label_binary'])\n",
    "party_label_crosstab.head(10).plot(kind='bar', stacked=True, color=['red', 'green'])\n",
    "plt.title('Real vs Fake by Party')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Fake', 'Real'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484a2838-7793-4cd4-946e-2aedfbb5775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 FEATURE ENGINEERING...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ✅ Created \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_features\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m additional features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_features\n\u001b[1;32m---> 23\u001b[0m train_final \u001b[38;5;241m=\u001b[39m create_additional_features(\u001b[43mtrain_clean\u001b[49m)\n\u001b[0;32m     24\u001b[0m test_final \u001b[38;5;241m=\u001b[39m create_additional_features(test_clean)\n\u001b[0;32m     25\u001b[0m valid_final \u001b[38;5;241m=\u001b[39m create_additional_features(valid_clean)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_clean' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔧 FEATURE ENGINEERING...\")\n",
    "\n",
    "def create_additional_features(df):\n",
    "    \"\"\"Create additional features for better model performance\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    df_features['has_question'] = df_features['clean_statement'].str.contains('\\?').astype(int)\n",
    "    df_features['has_exclamation'] = df_features['statement'].str.contains('!').astype(int)\n",
    "    df_features['has_quotes'] = df_features['statement'].str.contains('\"').astype(int)\n",
    "    df_features['uppercase_ratio'] = df_features['statement'].apply(\n",
    "        lambda x: sum(c.isupper() for c in str(x)) / max(len(str(x)), 1)\n",
    "    )\n",
    "    \n",
    "    df_features['avg_word_length'] = df_features['clean_statement'].apply(\n",
    "        lambda x: np.mean([len(word) for word in str(x).split()]) if str(x).split() else 0\n",
    "    )\n",
    "    \n",
    "    df_features['party_subject'] = df_features['party'] + '_' + df_features['subject']\n",
    "    \n",
    "    print(f\"  ✅ Created {len(df_features.columns) - len(df.columns)} additional features\")\n",
    "    return df_features\n",
    "\n",
    "train_final = create_additional_features(train_clean)\n",
    "test_final = create_additional_features(test_clean)\n",
    "valid_final = create_additional_features(valid_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5254d8dd-a4ff-44e3-9a63-83e14f5beabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 SAVING CLEANED DATA...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m💾 SAVING CLEANED DATA...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m important_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_statement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_quotes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muppercase_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word_length\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m available_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m important_columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m train_final\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     13\u001b[0m train_final[available_columns]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/train_clean.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m test_final[available_columns]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/test_clean.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m💾 SAVING CLEANED DATA...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m important_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_statement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_quotes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muppercase_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_word_length\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m available_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m important_columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_final\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     13\u001b[0m train_final[available_columns]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/train_clean.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m test_final[available_columns]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/test_clean.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_final' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n💾 SAVING CLEANED DATA...\")\n",
    "\n",
    "important_columns = [\n",
    "    'id', 'statement', 'clean_statement', 'label', 'label_binary',\n",
    "    'subject', 'speaker', 'party', 'job', 'state',\n",
    "    'subject_code', 'party_code', 'speaker_code', 'job_code', 'state_code',\n",
    "    'statement_length', 'word_count', 'has_question', 'has_exclamation',\n",
    "    'has_quotes', 'uppercase_ratio', 'avg_word_length'\n",
    "]\n",
    "\n",
    "available_columns = [col for col in important_columns if col in train_final.columns]\n",
    "\n",
    "train_final[available_columns].to_csv('../data/train_clean.csv', index=False)\n",
    "test_final[available_columns].to_csv('../data/test_clean.csv', index=False)\n",
    "valid_final[available_columns].to_csv('../data/valid_clean.csv', index=False)\n",
    "\n",
    "print(\"✅ Cleaned data saved successfully!\")\n",
    "print(f\"   📄 train_clean.csv: {len(train_final)} rows\")\n",
    "print(f\"   📄 test_clean.csv: {len(test_final)} rows\") \n",
    "print(f\"   📄 valid_clean.csv: {len(valid_final)} rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9dad38-d063-460a-a76d-92334432e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 CREATING DATA SUMMARY REPORT...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📋 CREATING DATA SUMMARY REPORT...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create comprehensive summary\u001b[39;00m\n\u001b[0;32m      4\u001b[0m summary_report \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal_Size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(train_df), \u001b[38;5;28mlen\u001b[39m(test_df), \u001b[38;5;28mlen\u001b[39m(valid_df)],\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClean_Size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_final\u001b[49m), \u001b[38;5;28mlen\u001b[39m(test_final), \u001b[38;5;28mlen\u001b[39m(valid_final)],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReal_News\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28msum\u001b[39m(train_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28msum\u001b[39m(test_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28msum\u001b[39m(valid_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m     ],\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFake_News\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28msum\u001b[39m(train_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28msum\u001b[39m(test_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28msum\u001b[39m(valid_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m     ],\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg_Length\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m     19\u001b[0m         train_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m     20\u001b[0m         test_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m     21\u001b[0m         valid_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     22\u001b[0m     ]\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     25\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(summary_report)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_final' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n📋 CREATING DATA SUMMARY REPORT...\")\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary_report = {\n",
    "    'Dataset': ['Training', 'Test', 'Validation'],\n",
    "    'Original_Size': [len(train_df), len(test_df), len(valid_df)],\n",
    "    'Clean_Size': [len(train_final), len(test_final), len(valid_final)],\n",
    "    'Real_News': [\n",
    "        sum(train_final['label_binary'] == 1),\n",
    "        sum(test_final['label_binary'] == 1), \n",
    "        sum(valid_final['label_binary'] == 1)\n",
    "    ],\n",
    "    'Fake_News': [\n",
    "        sum(train_final['label_binary'] == 0),\n",
    "        sum(test_final['label_binary'] == 0),\n",
    "        sum(valid_final['label_binary'] == 0)\n",
    "    ],\n",
    "    'Avg_Length': [\n",
    "        train_final['statement_length'].mean(),\n",
    "        test_final['statement_length'].mean(),\n",
    "        valid_final['statement_length'].mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_report)\n",
    "print(summary_df)\n",
    "\n",
    "# Save summary report\n",
    "summary_df.to_csv('../data/data_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n🎉 DATA CLEANING COMPLETE!\")\n",
    "print(\"📁 Files created for team:\")\n",
    "print(\"   - train_clean.csv\")\n",
    "print(\"   - test_clean.csv\") \n",
    "print(\"   - valid_clean.csv\")\n",
    "print(\"   - data_summary.csv\")\n",
    "print(\"\\n👥 Next: Share data/ folder with Member 2 (TF-IDF Model)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e5c4e-0202-424d-84be-cbdf04df5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📝 SAMPLE CLEANED DATA FOR TEAM REVIEW:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_data = train_final[['statement', 'clean_statement', 'label', 'label_binary', \n",
    "                          'party', 'subject', 'statement_length']].head(3)\n",
    "\n",
    "for idx, row in sample_data.iterrows():\n",
    "    print(f\"\\n🔍 Sample {idx + 1}:\")\n",
    "    print(f\"   Original: {row['statement'][:100]}...\")\n",
    "    print(f\"   Cleaned:  {row['clean_statement'][:100]}...\")\n",
    "    print(f\"   Label:    {row['label']} -> {row['label_binary']} ({'Real' if row['label_binary'] else 'Fake'})\")\n",
    "    print(f\"   Party:    {row['party']}\")\n",
    "    print(f\"   Subject:  {row['subject']}\")\n",
    "    print(f\"   Length:   {row['statement_length']} characters\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
