{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a567b15-2b85-4e93-9287-1a3ab424298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting BERT Model Development...\n",
      "üíª Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"3\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Starting BERT Model Development...\")\n",
    "print(f\"üíª Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ead004-dba9-49b6-8127-f8eec4844819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading cleaned data from Member 1...\n",
      "‚úÖ Data loaded successfully!\n",
      "Training: 10240 samples\n",
      "Test: 1267 samples\n",
      "Validation: 1284 samples\n",
      "\n",
      "‚ö° Using subset for faster training:\n",
      "   Training subset: 2000 samples\n",
      "   Test subset: 400 samples\n",
      "‚úÖ Sample created - Train: 2000, Test: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading cleaned data from Member 1...\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train_clean.csv')\n",
    "    test_df = pd.read_csv('../data/test_clean.csv')\n",
    "    valid_df = pd.read_csv('../data/valid_clean.csv')\n",
    "    \n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Training: {len(train_df)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    print(f\"Validation: {len(valid_df)} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Cleaned data not found!\")\n",
    "    print(\"Please run Member 1's notebook first (01_data_cleaning.ipynb)\")\n",
    "    exit()\n",
    "\n",
    "# For demonstration and speed, we'll use a subset of data\n",
    "# In production, you can use the full dataset\n",
    "SAMPLE_SIZE = 2000  # Adjust based on your computer's capability\n",
    "TEST_SIZE = 400\n",
    "\n",
    "print(f\"\\n‚ö° Using subset for faster training:\")\n",
    "print(f\"   Training subset: {SAMPLE_SIZE} samples\")\n",
    "print(f\"   Test subset: {TEST_SIZE} samples\")\n",
    "\n",
    "# Sample data (stratified to maintain label balance)\n",
    "train_sample = train_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLE_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_sample = test_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), TEST_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Sample created - Train: {len(train_sample)}, Test: {len(test_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56ec994-a5ac-4852-a402-af920a77e945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ INITIALIZING BERT MODEL...\n",
      "üì• Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BERT model initialized successfully!\n",
      "   Model: distilbert-base-uncased\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ INITIALIZING BERT MODEL...\")\n",
    "\n",
    "# Load pre-trained DistilBERT (faster than full BERT)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "print(\"üì• Loading tokenizer and model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ BERT model initialized successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec27968-d776-4328-830d-e43ba60306c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ TOKENIZING TEXT DATA...\n",
      "  üîÑ Tokenizing 2000 texts...\n",
      "  üîÑ Tokenizing 400 texts...\n",
      "‚úÖ Tokenization complete!\n",
      "   Max sequence length: 128\n",
      "   Training tokens shape: torch.Size([2000, 66])\n",
      "   Test tokens shape: torch.Size([400, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüî§ TOKENIZING TEXT DATA...\")\n",
    "\n",
    "def tokenize_data(texts, labels, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize text data for BERT input\n",
    "    \"\"\"\n",
    "    print(f\"  üîÑ Tokenizing {len(texts)} texts...\")\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return encodings, torch.tensor(labels.values)\n",
    "\n",
    "# Tokenize training data\n",
    "train_texts = train_sample['clean_statement'].fillna('')\n",
    "train_labels = train_sample['label_binary']\n",
    "\n",
    "train_encodings, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
    "\n",
    "# Tokenize test data  \n",
    "test_texts = test_sample['clean_statement'].fillna('')\n",
    "test_labels = test_sample['label_binary']\n",
    "\n",
    "test_encodings, test_labels_tensor = tokenize_data(test_texts, test_labels)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "print(f\"   Max sequence length: 128\")\n",
    "print(f\"   Training tokens shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"   Test tokens shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d64d32-a1ff-4757-8b92-0c9414e808e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Creating PyTorch datasets...\n",
      "‚úÖ Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for BERT training\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "print(\"üì¶ Creating PyTorch datasets...\")\n",
    "train_dataset = NewsDataset(train_encodings, train_labels_tensor)\n",
    "test_dataset = NewsDataset(test_encodings, test_labels_tensor)\n",
    "\n",
    "print(\"‚úÖ Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6117b3f0-c159-4e17-9af5-05374474831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öô CONFIGURING TRAINING PARAMETERS...\n",
      "‚úÖ Training configuration set!\n",
      "   Epochs: 3\n",
      "   Batch size: 8\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öô CONFIGURING TRAINING PARAMETERS...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../models/saved_bert_model', exist_ok=True)\n",
    "\n",
    "# Training arguments - UPDATED for newer Transformers version\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/bert_results',\n",
    "    num_train_epochs=3,                 # Number of training epochs\n",
    "    per_device_train_batch_size=8,      # Batch size (reduce if out of memory)\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    warmup_steps=100,                   # Warmup steps for learning rate\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    logging_dir='../models/bert_logs',  # Directory for storing logs\n",
    "    logging_steps=50,                   # Log every 50 steps\n",
    "    eval_strategy=\"steps\",              # CHANGED: evaluation_strategy ‚Üí eval_strategy\n",
    "    eval_steps=100,                     # Evaluation frequency\n",
    "    save_strategy=\"steps\",              # Save model every save_steps\n",
    "    save_steps=200,                     # Save frequency\n",
    "    load_best_model_at_end=True,        # Load best model at end\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,                     # Disable wandb logging\n",
    "    save_total_limit=2,                 # Keep only 2 best models\n",
    "    seed=42                             # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4781ff4-64b3-4a10-8444-00f5bacfadf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
