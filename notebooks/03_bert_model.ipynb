{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304f5314-669c-4c9d-a904-1387b733f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting BERT Model Development...\n",
      "üíª Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Starting BERT Model Development...\")\n",
    "print(f\"üíª Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e983ca-b1c4-4fdd-aa0e-2e3b74e82869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading cleaned data from Member 1...\n",
      "‚úÖ Data loaded successfully!\n",
      "Training: 10240 samples\n",
      "Test: 1267 samples\n",
      "Validation: 1284 samples\n",
      "\n",
      "‚ö° Using subset for faster training:\n",
      "   Training subset: 2000 samples\n",
      "   Test subset: 400 samples\n",
      "‚úÖ Sample created - Train: 2000, Test: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading cleaned data from Member 1...\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train_clean.csv')\n",
    "    test_df = pd.read_csv('../data/test_clean.csv')\n",
    "    valid_df = pd.read_csv('../data/valid_clean.csv')\n",
    "    \n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Training: {len(train_df)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    print(f\"Validation: {len(valid_df)} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Cleaned data not found!\")\n",
    "    print(\"Please run Member 1's notebook first (01_data_cleaning.ipynb)\")\n",
    "    exit()\n",
    "\n",
    "# For demonstration and speed, we'll use a subset of data\n",
    "# In production, you can use the full dataset\n",
    "SAMPLE_SIZE = 2000  # Adjust based on your computer's capability\n",
    "TEST_SIZE = 400\n",
    "\n",
    "print(f\"\\n‚ö° Using subset for faster training:\")\n",
    "print(f\"   Training subset: {SAMPLE_SIZE} samples\")\n",
    "print(f\"   Test subset: {TEST_SIZE} samples\")\n",
    "\n",
    "# Sample data (stratified to maintain label balance)\n",
    "train_sample = train_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLE_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_sample = test_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), TEST_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Sample created - Train: {len(train_sample)}, Test: {len(test_sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8841a-3dca-4636-8623-22ac8b0fbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ INITIALIZING BERT MODEL...\n",
      "üì• Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388d68b31d4b42c7892bbc7b1bdf1022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5385dc789e204f16bb25632c2610a891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc55897aa46a4b0184d5a715a058a766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe2dae3f83b4c199fc170ff2ad17cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a767e4f35f504b3998d3930ea016f7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nü§ñ INITIALIZING BERT MODEL...\")\n",
    "\n",
    "# Load pre-trained DistilBERT (faster than full BERT)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "print(\"üì• Loading tokenizer and model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ BERT model initialized successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77460ff-57a3-4af3-8393-f397d583d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ TOKENIZING TEXT DATA...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encodings, torch\u001b[38;5;241m.\u001b[39mtensor(labels\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Tokenize training data\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_texts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sample\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_statement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m train_encodings, train_labels_tensor \u001b[38;5;241m=\u001b[39m tokenize_data(train_texts, train_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sample' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nüî§ TOKENIZING TEXT DATA...\")\n",
    "\n",
    "def tokenize_data(texts, labels, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize text data for BERT input\n",
    "    \"\"\"\n",
    "    print(f\"  üîÑ Tokenizing {len(texts)} texts...\")\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return encodings, torch.tensor(labels.values)\n",
    "\n",
    "# Tokenize training data\n",
    "train_texts = train_sample['clean_statement'].fillna('')\n",
    "train_labels = train_sample['label_binary']\n",
    "\n",
    "train_encodings, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
    "\n",
    "# Tokenize test data  \n",
    "test_texts = test_sample['clean_statement'].fillna('')\n",
    "test_labels = test_sample['label_binary']\n",
    "\n",
    "test_encodings, test_labels_tensor = tokenize_data(test_texts, test_labels)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "print(f\"   Max sequence length: 128\")\n",
    "print(f\"   Training tokens shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"   Test tokens shape: {test_encodings['input_ids'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a8ec6-9f36-4c98-b399-c3506695226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for BERT training\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "print(\"üì¶ Creating PyTorch datasets...\")\n",
    "train_dataset = NewsDataset(train_encodings, train_labels_tensor)\n",
    "test_dataset = NewsDataset(test_encodings, test_labels_tensor)\n",
    "\n",
    "print(\"‚úÖ Datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9383fd-203d-4e23-9480-0ec5a6f42b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è CONFIGURING TRAINING PARAMETERS...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../models/saved_bert_model', exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/bert_results',\n",
    "    num_train_epochs=3,                 # Number of training epochs\n",
    "    per_device_train_batch_size=8,      # Batch size (reduce if out of memory)\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    warmup_steps=100,                   # Warmup steps for learning rate\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    logging_dir='../models/bert_logs',  # Directory for storing logs\n",
    "    logging_steps=50,                   # Log every 50 steps\n",
    "    evaluation_strategy=\"steps\",        # Evaluate every eval_steps\n",
    "    eval_steps=100,                     # Evaluation frequency\n",
    "    save_strategy=\"steps\",              # Save model every save_steps\n",
    "    save_steps=200,                     # Save frequency\n",
    "    load_best_model_at_end=True,        # Load best model at end\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,                     # Disable wandb logging\n",
    "    save_total_limit=2,                 # Keep only 2 best models\n",
    "    seed=42                             # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d166d-c5b7-44b6-921b-70c283b84598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    if not hasattr(tf, 'random'):\n",
    "        import tensorflow._api.v2.random as tf_random\n",
    "        tf.random = tf_random\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"\\nüéØ INITIALIZING TRAINER...\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de881fc6-3e1d-435a-9030-70e78d921021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ STARTING BERT TRAINING...\")\n",
    "print(\"‚è∞ This may take 10-30 minutes depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    training_results = trainer.train()\n",
    "    \n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"   Final training loss: {training_results.training_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüìä Evaluating on test set...\")\n",
    "    eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation complete!\")\n",
    "    for metric, value in eval_results.items():\n",
    "        if 'eval_' in metric:\n",
    "            print(f\"   {metric.replace('eval_', '').title()}: {value:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    print(\"üí° Try reducing batch size or sample size if out of memory\")\n",
    "    print(\"   You can also use CPU-only training by setting device='cpu'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec5769-204b-49b1-aaf2-3fc27ffa8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ SAVING BERT MODEL...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained('../models/saved_bert_model')\n",
    "tokenizer.save_pretrained('../models/saved_bert_model')\n",
    "\n",
    "print(\"‚úÖ BERT model saved successfully!\")\n",
    "print(\"üìÅ Files saved:\")\n",
    "print(\"   - ../models/saved_bert_model/ (directory)\")\n",
    "print(\"   - Model config, weights, and tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55fc9b-bb69-46b2-a773-b39fca680f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ TESTING BERT MODEL...\")\n",
    "\n",
    "def predict_with_bert(text, model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Make prediction using trained BERT model\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Loading saved model...\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('../models/saved_bert_model')\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('../models/saved_bert_model')\n",
    "        model.to(device)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions.max().item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'Real' if predicted_class == 1 else 'Fake',\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'Fake': predictions[0][0].item(),\n",
    "            'Real': predictions[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test with sample statements\n",
    "test_statements = [\n",
    "    \"The President announced new infrastructure spending today.\",\n",
    "    \"Scientists have proven that the Earth is actually flat!\",\n",
    "    \"Stock markets reached record highs following positive earnings reports.\",\n",
    "    \"Breaking: Aliens have invaded Earth and are demanding pizza!\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ TESTING BERT WITH SAMPLE STATEMENTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, statement in enumerate(test_statements, 1):\n",
    "    try:\n",
    "        result = predict_with_bert(statement, model, tokenizer)\n",
    "        print(f\"\\nüì∞ Test {i}:\")\n",
    "        print(f\"   Statement: {statement}\")\n",
    "        print(f\"   ü§ñ BERT Prediction: {result['prediction']}\")\n",
    "        print(f\"   üìä Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"   üî¥ Fake probability: {result['probabilities']['Fake']:.3f}\")\n",
    "        print(f\"   üü¢ Real probability: {result['probabilities']['Real']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing statement {i}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e43b36-57a7-4b15-807c-2337ba503fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà BATCH PREDICTION ANALYSIS...\")\n",
    "\n",
    "# Get predictions for entire test set\n",
    "def get_bert_predictions(dataset, model, batch_size=16):\n",
    "    \"\"\"\n",
    "    Get predictions for entire dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(dataset)} samples in batches of {batch_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(torch.argmax(predictions, dim=-1).cpu().numpy())\n",
    "            all_probabilities.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  üìä Processed {(i + 1) * batch_size} samples...\")\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_probabilities)\n",
    "\n",
    "try:\n",
    "    # Get BERT predictions\n",
    "    bert_predictions, bert_probabilities = get_bert_predictions(test_dataset, model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bert_accuracy = accuracy_score(test_labels_tensor.cpu().numpy(), bert_predictions)\n",
    "    \n",
    "    print(f\"\\n‚úÖ BERT Model Performance:\")\n",
    "    print(f\"   Test Accuracy: {bert_accuracy:.3f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    print(classification_report(test_labels_tensor.cpu().numpy(), bert_predictions, \n",
    "                              target_names=['Fake', 'Real']))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in batch prediction: {str(e)}\")\n",
    "    print(\"üí° Try reducing batch_size or sample_size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03807191-da74-4876-8d57-3275e356bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä CREATING VISUALIZATIONS...\")\n",
    "\n",
    "try:\n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 3, 1)\n",
    "    cm = confusion_matrix(test_labels_tensor.cpu().numpy(), bert_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "    plt.title('BERT Confusion Matrix')\n",
    "    \n",
    "    # Prediction Confidence Distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    confidence_scores = np.max(bert_probabilities, axis=1)\n",
    "    plt.hist(confidence_scores, bins=30, alpha=0.7, color='green')\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Confidence by True Label\n",
    "    plt.subplot(2, 3, 3)\n",
    "    true_labels = test_labels_tensor.cpu().numpy()\n",
    "    real_confidence = confidence_scores[true_labels == 1]\n",
    "    fake_confidence = confidence_scores[true_labels == 0]\n",
    "    \n",
    "    plt.hist(real_confidence, bins=20, alpha=0.7, label='Real News', color='green')\n",
    "    plt.hist(fake_confidence, bins=20, alpha=0.7, label='Fake News', color='red')\n",
    "    plt.title('Confidence by True Label')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Correct vs Incorrect Predictions\n",
    "    plt.subplot(2, 3, 4)\n",
    "    correct_mask = (bert_predictions == true_labels)\n",
    "    correct_conf = confidence_scores[correct_mask]\n",
    "    incorrect_conf = confidence_scores[~correct_mask]\n",
    "    \n",
    "    plt.hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='blue')\n",
    "    plt.hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='orange')\n",
    "    plt.title('Confidence: Correct vs Incorrect')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Label Distribution\n",
    "    plt.subplot(2, 3, 5)\n",
    "    labels = ['Fake', 'Real']\n",
    "    true_counts = np.bincount(true_labels)\n",
    "    pred_counts = np.bincount(bert_predictions)\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, true_counts, width, label='True', alpha=0.8)\n",
    "    plt.bar(x + width/2, pred_counts, width, label='Predicted', alpha=0.8)\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('True vs Predicted Distribution')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Performance by Confidence Threshold\n",
    "    plt.subplot(2, 3, 6)\n",
    "    thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "    accuracies = []\n",
    "    sample_sizes = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        high_conf_mask = confidence_scores >= threshold\n",
    "        if sum(high_conf_mask) > 0:\n",
    "            high_conf_acc = accuracy_score(\n",
    "                true_labels[high_conf_mask], \n",
    "                bert_predictions[high_conf_mask]\n",
    "            )\n",
    "            accuracies.append(high_conf_acc)\n",
    "            sample_sizes.append(sum(high_conf_mask))\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "            sample_sizes.append(0)\n",
    "    \n",
    "    plt.plot(thresholds, accuracies, 'b-o', label='Accuracy')\n",
    "    plt.axhline(y=bert_accuracy, color='r', linestyle='--', label='Overall Accuracy')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Confidence Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Visualization error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58186a-4cce-4a1f-be7c-12529fef5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ COMPARING WITH TF-IDF MODEL...\")\n",
    "\n",
    "try:\n",
    "    # Load Member 2's TF-IDF model\n",
    "    tfidf_model = joblib.load('../models/tfidf_model.pkl')\n",
    "    tfidf_vectorizer = joblib.load('../models/tfidf_vectorizer.pkl')\n",
    "    \n",
    "    # Get TF-IDF predictions on same test data\n",
    "    test_texts_tfidf = test_sample['clean_statement'].fillna('')\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_texts_tfidf)\n",
    "    tfidf_predictions = tfidf_model.predict(X_test_tfidf)\n",
    "    tfidf_accuracy = accuracy_score(test_labels_tensor.cpu().numpy(), tfidf_predictions)\n",
    "    \n",
    "    print(\"‚úÖ Model comparison:\")\n",
    "    print(f\"   TF-IDF Accuracy: {tfidf_accuracy:.3f}\")\n",
    "    print(f\"   BERT Accuracy:   {bert_accuracy:.3f}\")\n",
    "    print(f\"   Improvement:     {bert_accuracy - tfidf_accuracy:.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    models = ['TF-IDF', 'BERT']\n",
    "    accuracies = [tfidf_accuracy, bert_accuracy]\n",
    "    colors = ['lightblue', 'darkgreen']\n",
    "    \n",
    "    bars = plt.bar(models, accuracies, color=colors)\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Agreement between models\n",
    "    agreement = (tfidf_predictions == bert_predictions)\n",
    "    agreement_rate = sum(agreement) / len(agreement)\n",
    "    \n",
    "    labels = ['Disagree', 'Agree']\n",
    "    sizes = [1-agreement_rate, agreement_rate]\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "    plt.title(f'Model Agreement\\n({agreement_rate:.1%})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è TF-IDF model not found. Run Member 2's notebook first for comparison.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Comparison error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2f870-c965-4f49-b0bb-2c978e265f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç ERROR ANALYSIS...\")\n",
    "\n",
    "try:\n",
    "    # Find misclassified examples\n",
    "    true_labels = test_labels_tensor.cpu().numpy()\n",
    "    incorrect_mask = (bert_predictions != true_labels)\n",
    "    \n",
    "    if sum(incorrect_mask) > 0:\n",
    "        print(f\"üìä Found {sum(incorrect_mask)} misclassified examples:\")\n",
    "        \n",
    "        # Get misclassified examples\n",
    "        misclassified_indices = np.where(incorrect_mask)[0]\n",
    "        sample_errors = misclassified_indices[:5]  # Show first 5 errors\n",
    "        \n",
    "        print(\"\\n‚ùå Sample Misclassifications:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, idx in enumerate(sample_errors):\n",
    "            original_idx = test_sample.iloc[idx].name\n",
    "            statement = test_sample.iloc[idx]['statement']\n",
    "            true_label = 'Real' if true_labels[idx] == 1 else 'Fake'\n",
    "            pred_label = 'Real' if bert_predictions[idx] == 1 else 'Fake'\n",
    "            confidence = np.max(bert_probabilities[idx])\n",
    "            \n",
    "            print(f\"\\nüîç Error {i+1}:\")\n",
    "            print(f\"   Statement: {statement[:100]}...\")\n",
    "            print(f\"   True Label: {true_label}\")\n",
    "            print(f\"   Predicted: {pred_label}\")\n",
    "            print(f\"   Confidence: {confidence:.3f}\")\n",
    "            print(f\"   Subject: {test_sample.iloc[idx].get('subject', 'N/A')}\")\n",
    "            print(f\"   Party: {test_sample.iloc[idx].get('party', 'N/A')}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    # Analysis by subject\n",
    "    test_sample_with_pred = test_sample.copy()\n",
    "    test_sample_with_pred['bert_prediction'] = bert_predictions\n",
    "    test_sample_with_pred['correct'] = (bert_predictions == true_labels)\n",
    "    \n",
    "    subject_accuracy = test_sample_with_pred.groupby('subject')['correct'].agg(['mean', 'count'])\n",
    "    subject_accuracy = subject_accuracy[subject_accuracy['count'] >= 5]  # Only subjects with 5+ samples\n",
    "    subject_accuracy = subject_accuracy.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Accuracy by Subject (min 5 samples):\")\n",
    "    print(subject_accuracy.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error analysis failed: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6185849-bb32-441a-9fde-a41809789e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÑ CREATING RESULTS SUMMARY...\")\n",
    "\n",
    "# Create comprehensive results dictionary\n",
    "bert_results = {\n",
    "    'model_type': 'DistilBERT',\n",
    "    'test_accuracy': bert_accuracy,\n",
    "    'sample_size': len(test_sample),\n",
    "    'training_epochs': training_args.num_train_epochs,\n",
    "    'batch_size': training_args.per_device_train_batch_size,\n",
    "    'device_used': str(device),\n",
    "    'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame([bert_results])\n",
    "results_df.to_csv('../models/bert_model_results.csv', index=False)\n",
    "\n",
    "# Save predictions for Member 4 (hybrid model)\n",
    "predictions_df = pd.DataFrame({\n",
    "    'test_index': range(len(bert_predictions)),\n",
    "    'true_label': test_labels_tensor.cpu().numpy(),\n",
    "    'bert_prediction': bert_predictions,\n",
    "    'bert_confidence': np.max(bert_probabilities, axis=1),\n",
    "    'fake_probability': bert_probabilities[:, 0],\n",
    "    'real_probability': bert_probabilities[:, 1]\n",
    "})\n",
    "predictions_df.to_csv('../models/bert_predictions.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Results saved successfully!\")\n",
    "print(\"\\nüéâ BERT MODEL DEVELOPMENT COMPLETE!\")\n",
    "print(\"üìÅ Files created for team:\")\n",
    "print(\"   - ../models/saved_bert_model/ (complete model)\")\n",
    "print(\"   - ../models/bert_model_results.csv\")\n",
    "print(\"   - ../models/bert_predictions.csv\")\n",
    "print(\"\\nüë• Next: Member 4 can use BERT features for hybrid model!\")\n",
    "print(\"üí° Member 5 can integrate BERT into the Streamlit app!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc9b0-a6f7-4147-bb36-81ed353068ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüõ†Ô∏è CREATING UTILITY FUNCTIONS FOR TEAM...\")\n",
    "\n",
    "# Save utility functions for other members to use\n",
    "bert_utils_code = '''\n",
    "\"\"\"\n",
    "BERT Model Utilities for Team Integration\n",
    "Created by Member 3\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "def load_bert_model(model_path='../models/saved_bert_model'):\n",
    "    \"\"\"Load saved BERT model and tokenizer\"\"\"\n",
    "    try:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        return model, tokenizer, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BERT model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_with_bert_simple(text, model_path='../models/saved_bert_model'):\n",
    "    \"\"\"Simple prediction function for Streamlit app\"\"\"\n",
    "    model, tokenizer, device = load_bert_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    # Tokenize and predict\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                      padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions.max().item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': predictions[0].cpu().numpy()\n",
    "    }\n",
    "\n",
    "def get_bert_embeddings(texts, model_path='../models/saved_bert_model'):\n",
    "    \"\"\"Get BERT embeddings for hybrid model (Member 4)\"\"\"\n",
    "    model, tokenizer, device = load_bert_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                          padding=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            # Use the [CLS] token embedding from last hidden state\n",
    "            embedding = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding.squeeze())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "'''\n",
    "\n",
    "# Write utility functions to file\n",
    "with open('../utils/bert_utils.py', 'w') as f:\n",
    "    f.write(bert_utils_code)\n",
    "\n",
    "print(\"‚úÖ BERT utilities saved to ../utils/bert_utils.py\")\n",
    "print(\"üéØ Other team members can now import and use BERT functions!\")\n",
    "\n",
    "print(f\"\\nüèÅ MEMBER 3 BERT DEVELOPMENT SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ BERT model trained and saved\")\n",
    "print(f\"‚úÖ Test accuracy achieved: {bert_accuracy:.3f}\")\n",
    "print(f\"‚úÖ Model comparison with TF-IDF completed\")\n",
    "print(f\"‚úÖ Error analysis and visualizations created\")\n",
    "print(f\"‚úÖ Utility functions created for team integration\")\n",
    "print(f\"‚úÖ All files saved for Member 4 (hybrid) and Member 5 (app)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
