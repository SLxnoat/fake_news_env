{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304f5314-669c-4c9d-a904-1387b733f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting BERT Model Development...\n",
      "💻 Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Starting BERT Model Development...\")\n",
    "print(f\"💻 Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e983ca-b1c4-4fdd-aa0e-2e3b74e82869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading cleaned data from Member 1...\n",
      "❌ Cleaned data not found!\n",
      "Please run Member 1's notebook first (01_data_cleaning.ipynb)\n",
      "\n",
      "⚡ Using subset for faster training:\n",
      "   Training subset: 2000 samples\n",
      "   Test subset: 400 samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Test subset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Sample data (stratified to maintain label balance)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x), SAMPLE_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     30\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x), TEST_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Sample created - Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"📊 Loading cleaned data from Member 1...\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train_clean.csv')\n",
    "    test_df = pd.read_csv('../data/test_clean.csv')\n",
    "    valid_df = pd.read_csv('../data/valid_clean.csv')\n",
    "    \n",
    "    print(\"✅ Data loaded successfully!\")\n",
    "    print(f\"Training: {len(train_df)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    print(f\"Validation: {len(valid_df)} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Cleaned data not found!\")\n",
    "    print(\"Please run Member 1's notebook first (01_data_cleaning.ipynb)\")\n",
    "    exit()\n",
    "\n",
    "# For demonstration and speed, we'll use a subset of data\n",
    "# In production, you can use the full dataset\n",
    "SAMPLE_SIZE = 2000  # Adjust based on your computer's capability\n",
    "TEST_SIZE = 400\n",
    "\n",
    "print(f\"\\n⚡ Using subset for faster training:\")\n",
    "print(f\"   Training subset: {SAMPLE_SIZE} samples\")\n",
    "print(f\"   Test subset: {TEST_SIZE} samples\")\n",
    "\n",
    "# Sample data (stratified to maintain label balance)\n",
    "train_sample = train_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLE_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_sample = test_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), TEST_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Sample created - Train: {len(train_sample)}, Test: {len(test_sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c8841a-3dca-4636-8623-22ac8b0fbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 INITIALIZING BERT MODEL...\n",
      "📥 Loading tokenizer and model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DistilBertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📥 Loading tokenizer and model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      9\u001b[0m     model_name, \n\u001b[0;32m     10\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Binary classification\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Move model to device\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DistilBertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n🤖 INITIALIZING BERT MODEL...\")\n",
    "\n",
    "# Load pre-trained DistilBERT (faster than full BERT)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "print(\"📥 Loading tokenizer and model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ BERT model initialized successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77460ff-57a3-4af3-8393-f397d583d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔤 TOKENIZING TEXT DATA...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encodings, torch\u001b[38;5;241m.\u001b[39mtensor(labels\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Tokenize training data\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_texts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sample\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_statement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_binary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m train_encodings, train_labels_tensor \u001b[38;5;241m=\u001b[39m tokenize_data(train_texts, train_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sample' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔤 TOKENIZING TEXT DATA...\")\n",
    "\n",
    "def tokenize_data(texts, labels, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize text data for BERT input\n",
    "    \"\"\"\n",
    "    print(f\"  🔄 Tokenizing {len(texts)} texts...\")\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return encodings, torch.tensor(labels.values)\n",
    "\n",
    "# Tokenize training data\n",
    "train_texts = train_sample['clean_statement'].fillna('')\n",
    "train_labels = train_sample['label_binary']\n",
    "\n",
    "train_encodings, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
    "\n",
    "# Tokenize test data  \n",
    "test_texts = test_sample['clean_statement'].fillna('')\n",
    "test_labels = test_sample['label_binary']\n",
    "\n",
    "test_encodings, test_labels_tensor = tokenize_data(test_texts, test_labels)\n",
    "\n",
    "print(\"✅ Tokenization complete!\")\n",
    "print(f\"   Max sequence length: 128\")\n",
    "print(f\"   Training tokens shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"   Test tokens shape: {test_encodings['input_ids'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85a8ec6-9f36-4c98-b399-c3506695226e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNewsDataset\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Custom Dataset class for BERT training\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encodings, labels):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for BERT training\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "print(\"📦 Creating PyTorch datasets...\")\n",
    "train_dataset = NewsDataset(train_encodings, train_labels_tensor)\n",
    "test_dataset = NewsDataset(test_encodings, test_labels_tensor)\n",
    "\n",
    "print(\"✅ Datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9383fd-203d-4e23-9480-0ec5a6f42b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ CONFIGURING TRAINING PARAMETERS...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⚙️ CONFIGURING TRAINING PARAMETERS...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create models directory\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/saved_bert_model\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n⚙️ CONFIGURING TRAINING PARAMETERS...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../models/saved_bert_model', exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/bert_results',\n",
    "    num_train_epochs=3,                 # Number of training epochs\n",
    "    per_device_train_batch_size=8,      # Batch size (reduce if out of memory)\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    warmup_steps=100,                   # Warmup steps for learning rate\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    logging_dir='../models/bert_logs',  # Directory for storing logs\n",
    "    logging_steps=50,                   # Log every 50 steps\n",
    "    evaluation_strategy=\"steps\",        # Evaluate every eval_steps\n",
    "    eval_steps=100,                     # Evaluation frequency\n",
    "    save_strategy=\"steps\",              # Save model every save_steps\n",
    "    save_steps=200,                     # Save frequency\n",
    "    load_best_model_at_end=True,        # Load best model at end\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,                     # Disable wandb logging\n",
    "    save_total_limit=2,                 # Keep only 2 best models\n",
    "    seed=42                             # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"✅ Training configuration set!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989d166d-c5b7-44b6-921b-70c283b84598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 INITIALIZING TRAINER...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎯 INITIALIZING TRAINER...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m(\n\u001b[0;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     27\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Trainer initialized successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"\\n🎯 INITIALIZING TRAINER...\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de881fc6-3e1d-435a-9030-70e78d921021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 STARTING BERT TRAINING...\n",
      "⏰ This may take 10-30 minutes depending on your hardware...\n",
      "❌ Training failed: name 'trainer' is not defined\n",
      "💡 Try reducing batch size or sample size if out of memory\n",
      "   You can also use CPU-only training by setting device='cpu'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🚀 STARTING BERT TRAINING...\")\n",
    "print(\"⏰ This may take 10-30 minutes depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    training_results = trainer.train()\n",
    "    \n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    print(f\"   Final training loss: {training_results.training_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n📊 Evaluating on test set...\")\n",
    "    eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    print(\"✅ Evaluation complete!\")\n",
    "    for metric, value in eval_results.items():\n",
    "        if 'eval_' in metric:\n",
    "            print(f\"   {metric.replace('eval_', '').title()}: {value:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {str(e)}\")\n",
    "    print(\"💡 Try reducing batch size or sample size if out of memory\")\n",
    "    print(\"   You can also use CPU-only training by setting device='cpu'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ec5769-204b-49b1-aaf2-3fc27ffa8b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 SAVING BERT MODEL...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m💾 SAVING BERT MODEL...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save model and tokenizer\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/saved_bert_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/saved_bert_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ BERT model saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n💾 SAVING BERT MODEL...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained('../models/saved_bert_model')\n",
    "tokenizer.save_pretrained('../models/saved_bert_model')\n",
    "\n",
    "print(\"✅ BERT model saved successfully!\")\n",
    "print(\"📁 Files saved:\")\n",
    "print(\"   - ../models/saved_bert_model/ (directory)\")\n",
    "print(\"   - Model config, weights, and tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e55fc9b-bb69-46b2-a773-b39fca680f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 TESTING BERT MODEL...\n",
      "\n",
      "🧪 TESTING BERT WITH SAMPLE STATEMENTS:\n",
      "======================================================================\n",
      "❌ Error testing statement 1: name 'model' is not defined\n",
      "❌ Error testing statement 2: name 'model' is not defined\n",
      "❌ Error testing statement 3: name 'model' is not defined\n",
      "❌ Error testing statement 4: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧪 TESTING BERT MODEL...\")\n",
    "\n",
    "def predict_with_bert(text, model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Make prediction using trained BERT model\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Loading saved model...\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('../models/saved_bert_model')\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('../models/saved_bert_model')\n",
    "        model.to(device)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions.max().item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'Real' if predicted_class == 1 else 'Fake',\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'Fake': predictions[0][0].item(),\n",
    "            'Real': predictions[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test with sample statements\n",
    "test_statements = [\n",
    "    \"The President announced new infrastructure spending today.\",\n",
    "    \"Scientists have proven that the Earth is actually flat!\",\n",
    "    \"Stock markets reached record highs following positive earnings reports.\",\n",
    "    \"Breaking: Aliens have invaded Earth and are demanding pizza!\"\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 TESTING BERT WITH SAMPLE STATEMENTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, statement in enumerate(test_statements, 1):\n",
    "    try:\n",
    "        result = predict_with_bert(statement, model, tokenizer)\n",
    "        print(f\"\\n📰 Test {i}:\")\n",
    "        print(f\"   Statement: {statement}\")\n",
    "        print(f\"   🤖 BERT Prediction: {result['prediction']}\")\n",
    "        print(f\"   📊 Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"   🔴 Fake probability: {result['probabilities']['Fake']:.3f}\")\n",
    "        print(f\"   🟢 Real probability: {result['probabilities']['Real']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing statement {i}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e43b36-57a7-4b15-807c-2337ba503fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 BATCH PREDICTION ANALYSIS...\n",
      "❌ Error in batch prediction: name 'test_dataset' is not defined\n",
      "💡 Try reducing batch_size or sample_size\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📈 BATCH PREDICTION ANALYSIS...\")\n",
    "\n",
    "# Get predictions for entire test set\n",
    "def get_bert_predictions(dataset, model, batch_size=16):\n",
    "    \"\"\"\n",
    "    Get predictions for entire dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"🔄 Processing {len(dataset)} samples in batches of {batch_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(torch.argmax(predictions, dim=-1).cpu().numpy())\n",
    "            all_probabilities.extend(predictions.cpu().numpy())\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  📊 Processed {(i + 1) * batch_size} samples...\")\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_probabilities)\n",
    "\n",
    "try:\n",
    "    # Get BERT predictions\n",
    "    bert_predictions, bert_probabilities = get_bert_predictions(test_dataset, model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bert_accuracy = accuracy_score(test_labels_tensor.cpu().numpy(), bert_predictions)\n",
    "    \n",
    "    print(f\"\\n✅ BERT Model Performance:\")\n",
    "    print(f\"   Test Accuracy: {bert_accuracy:.3f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\n📊 Detailed Classification Report:\")\n",
    "    print(classification_report(test_labels_tensor.cpu().numpy(), bert_predictions, \n",
    "                              target_names=['Fake', 'Real']))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in batch prediction: {str(e)}\")\n",
    "    print(\"💡 Try reducing batch_size or sample_size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03807191-da74-4876-8d57-3275e356bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CREATING VISUALIZATIONS...\n",
      "⚠️ Visualization error: name 'plt' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 CREATING VISUALIZATIONS...\")\n",
    "\n",
    "try:\n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 3, 1)\n",
    "    cm = confusion_matrix(test_labels_tensor.cpu().numpy(), bert_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "    plt.title('BERT Confusion Matrix')\n",
    "    \n",
    "    # Prediction Confidence Distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    confidence_scores = np.max(bert_probabilities, axis=1)\n",
    "    plt.hist(confidence_scores, bins=30, alpha=0.7, color='green')\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Confidence by True Label\n",
    "    plt.subplot(2, 3, 3)\n",
    "    true_labels = test_labels_tensor.cpu().numpy()\n",
    "    real_confidence = confidence_scores[true_labels == 1]\n",
    "    fake_confidence = confidence_scores[true_labels == 0]\n",
    "    \n",
    "    plt.hist(real_confidence, bins=20, alpha=0.7, label='Real News', color='green')\n",
    "    plt.hist(fake_confidence, bins=20, alpha=0.7, label='Fake News', color='red')\n",
    "    plt.title('Confidence by True Label')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Correct vs Incorrect Predictions\n",
    "    plt.subplot(2, 3, 4)\n",
    "    correct_mask = (bert_predictions == true_labels)\n",
    "    correct_conf = confidence_scores[correct_mask]\n",
    "    incorrect_conf = confidence_scores[~correct_mask]\n",
    "    \n",
    "    plt.hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='blue')\n",
    "    plt.hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='orange')\n",
    "    plt.title('Confidence: Correct vs Incorrect')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Label Distribution\n",
    "    plt.subplot(2, 3, 5)\n",
    "    labels = ['Fake', 'Real']\n",
    "    true_counts = np.bincount(true_labels)\n",
    "    pred_counts = np.bincount(bert_predictions)\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, true_counts, width, label='True', alpha=0.8)\n",
    "    plt.bar(x + width/2, pred_counts, width, label='Predicted', alpha=0.8)\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('True vs Predicted Distribution')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Performance by Confidence Threshold\n",
    "    plt.subplot(2, 3, 6)\n",
    "    thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "    accuracies = []\n",
    "    sample_sizes = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        high_conf_mask = confidence_scores >= threshold\n",
    "        if sum(high_conf_mask) > 0:\n",
    "            high_conf_acc = accuracy_score(\n",
    "                true_labels[high_conf_mask], \n",
    "                bert_predictions[high_conf_mask]\n",
    "            )\n",
    "            accuracies.append(high_conf_acc)\n",
    "            sample_sizes.append(sum(high_conf_mask))\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "            sample_sizes.append(0)\n",
    "    \n",
    "    plt.plot(thresholds, accuracies, 'b-o', label='Accuracy')\n",
    "    plt.axhline(y=bert_accuracy, color='r', linestyle='--', label='Overall Accuracy')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Confidence Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Visualization error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e58186a-4cce-4a1f-be7c-12529fef5149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 COMPARING WITH TF-IDF MODEL...\n",
      "⚠️ Comparison error: name 'joblib' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔄 COMPARING WITH TF-IDF MODEL...\")\n",
    "\n",
    "try:\n",
    "    # Load Member 2's TF-IDF model\n",
    "    tfidf_model = joblib.load('../models/tfidf_model.pkl')\n",
    "    tfidf_vectorizer = joblib.load('../models/tfidf_vectorizer.pkl')\n",
    "    \n",
    "    # Get TF-IDF predictions on same test data\n",
    "    test_texts_tfidf = test_sample['clean_statement'].fillna('')\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_texts_tfidf)\n",
    "    tfidf_predictions = tfidf_model.predict(X_test_tfidf)\n",
    "    tfidf_accuracy = accuracy_score(test_labels_tensor.cpu().numpy(), tfidf_predictions)\n",
    "    \n",
    "    print(\"✅ Model comparison:\")\n",
    "    print(f\"   TF-IDF Accuracy: {tfidf_accuracy:.3f}\")\n",
    "    print(f\"   BERT Accuracy:   {bert_accuracy:.3f}\")\n",
    "    print(f\"   Improvement:     {bert_accuracy - tfidf_accuracy:.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    models = ['TF-IDF', 'BERT']\n",
    "    accuracies = [tfidf_accuracy, bert_accuracy]\n",
    "    colors = ['lightblue', 'darkgreen']\n",
    "    \n",
    "    bars = plt.bar(models, accuracies, color=colors)\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Agreement between models\n",
    "    agreement = (tfidf_predictions == bert_predictions)\n",
    "    agreement_rate = sum(agreement) / len(agreement)\n",
    "    \n",
    "    labels = ['Disagree', 'Agree']\n",
    "    sizes = [1-agreement_rate, agreement_rate]\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "    plt.title(f'Model Agreement\\n({agreement_rate:.1%})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ TF-IDF model not found. Run Member 2's notebook first for comparison.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Comparison error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f2f870-c965-4f49-b0bb-2c978e265f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 ERROR ANALYSIS...\n",
      "⚠️ Error analysis failed: name 'test_labels_tensor' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔍 ERROR ANALYSIS...\")\n",
    "\n",
    "try:\n",
    "    # Find misclassified examples\n",
    "    true_labels = test_labels_tensor.cpu().numpy()\n",
    "    incorrect_mask = (bert_predictions != true_labels)\n",
    "    \n",
    "    if sum(incorrect_mask) > 0:\n",
    "        print(f\"📊 Found {sum(incorrect_mask)} misclassified examples:\")\n",
    "        \n",
    "        # Get misclassified examples\n",
    "        misclassified_indices = np.where(incorrect_mask)[0]\n",
    "        sample_errors = misclassified_indices[:5]  # Show first 5 errors\n",
    "        \n",
    "        print(\"\\n❌ Sample Misclassifications:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, idx in enumerate(sample_errors):\n",
    "            original_idx = test_sample.iloc[idx].name\n",
    "            statement = test_sample.iloc[idx]['statement']\n",
    "            true_label = 'Real' if true_labels[idx] == 1 else 'Fake'\n",
    "            pred_label = 'Real' if bert_predictions[idx] == 1 else 'Fake'\n",
    "            confidence = np.max(bert_probabilities[idx])\n",
    "            \n",
    "            print(f\"\\n🔍 Error {i+1}:\")\n",
    "            print(f\"   Statement: {statement[:100]}...\")\n",
    "            print(f\"   True Label: {true_label}\")\n",
    "            print(f\"   Predicted: {pred_label}\")\n",
    "            print(f\"   Confidence: {confidence:.3f}\")\n",
    "            print(f\"   Subject: {test_sample.iloc[idx].get('subject', 'N/A')}\")\n",
    "            print(f\"   Party: {test_sample.iloc[idx].get('party', 'N/A')}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    # Analysis by subject\n",
    "    test_sample_with_pred = test_sample.copy()\n",
    "    test_sample_with_pred['bert_prediction'] = bert_predictions\n",
    "    test_sample_with_pred['correct'] = (bert_predictions == true_labels)\n",
    "    \n",
    "    subject_accuracy = test_sample_with_pred.groupby('subject')['correct'].agg(['mean', 'count'])\n",
    "    subject_accuracy = subject_accuracy[subject_accuracy['count'] >= 5]  # Only subjects with 5+ samples\n",
    "    subject_accuracy = subject_accuracy.sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📊 Accuracy by Subject (min 5 samples):\")\n",
    "    print(subject_accuracy.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error analysis failed: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6185849-bb32-441a-9fde-a41809789e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 CREATING RESULTS SUMMARY...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bert_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📄 CREATING RESULTS SUMMARY...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create comprehensive results dictionary\u001b[39;00m\n\u001b[0;32m      4\u001b[0m bert_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistilBERT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbert_accuracy\u001b[49m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(test_sample),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: training_args\u001b[38;5;241m.\u001b[39mnum_train_epochs,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: training_args\u001b[38;5;241m.\u001b[39mper_device_train_batch_size,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_used\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(device),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainable_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m     16\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([bert_results])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n📄 CREATING RESULTS SUMMARY...\")\n",
    "\n",
    "# Create comprehensive results dictionary\n",
    "bert_results = {\n",
    "    'model_type': 'DistilBERT',\n",
    "    'test_accuracy': bert_accuracy,\n",
    "    'sample_size': len(test_sample),\n",
    "    'training_epochs': training_args.num_train_epochs,\n",
    "    'batch_size': training_args.per_device_train_batch_size,\n",
    "    'device_used': str(device),\n",
    "    'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame([bert_results])\n",
    "results_df.to_csv('../models/bert_model_results.csv', index=False)\n",
    "\n",
    "# Save predictions for Member 4 (hybrid model)\n",
    "predictions_df = pd.DataFrame({\n",
    "    'test_index': range(len(bert_predictions)),\n",
    "    'true_label': test_labels_tensor.cpu().numpy(),\n",
    "    'bert_prediction': bert_predictions,\n",
    "    'bert_confidence': np.max(bert_probabilities, axis=1),\n",
    "    'fake_probability': bert_probabilities[:, 0],\n",
    "    'real_probability': bert_probabilities[:, 1]\n",
    "})\n",
    "predictions_df.to_csv('../models/bert_predictions.csv', index=False)\n",
    "\n",
    "print(\"✅ Results saved successfully!\")\n",
    "print(\"\\n🎉 BERT MODEL DEVELOPMENT COMPLETE!\")\n",
    "print(\"📁 Files created for team:\")\n",
    "print(\"   - ../models/saved_bert_model/ (complete model)\")\n",
    "print(\"   - ../models/bert_model_results.csv\")\n",
    "print(\"   - ../models/bert_predictions.csv\")\n",
    "print(\"\\n👥 Next: Member 4 can use BERT features for hybrid model!\")\n",
    "print(\"💡 Member 5 can integrate BERT into the Streamlit app!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3bc9b0-a6f7-4147-bb36-81ed353068ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🛠️ CREATING UTILITY FUNCTIONS FOR TEAM...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../utils/bert_utils.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 74\u001b[0m\n\u001b[0;32m      4\u001b[0m bert_utils_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mBERT Model Utilities for Team Integration\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124m    return np.array(embeddings)\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Write utility functions to file\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../utils/bert_utils.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     75\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(bert_utils_code)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ BERT utilities saved to ../utils/bert_utils.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\fake_news_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../utils/bert_utils.py'"
     ]
    }
   ],
   "source": [
    "print(\"\\n🛠️ CREATING UTILITY FUNCTIONS FOR TEAM...\")\n",
    "\n",
    "# Save utility functions for other members to use\n",
    "bert_utils_code = '''\n",
    "\"\"\"\n",
    "BERT Model Utilities for Team Integration\n",
    "Created by Member 3\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "def load_bert_model(model_path='../models/saved_bert_model'):\n",
    "    \"\"\"Load saved BERT model and tokenizer\"\"\"\n",
    "    try:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        return model, tokenizer, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BERT model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_with_bert_simple(text, model_path='../models/saved_bert_model'):\n",
    "    \"\"\"Simple prediction function for Streamlit app\"\"\"\n",
    "    model, tokenizer, device = load_bert_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    # Tokenize and predict\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                      padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions.max().item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': predictions[0].cpu().numpy()\n",
    "    }\n",
    "\n",
    "def get_bert_embeddings(texts, model_path='../models/saved_bert_model'):\n",
    "    \"\"\"Get BERT embeddings for hybrid model (Member 4)\"\"\"\n",
    "    model, tokenizer, device = load_bert_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                          padding=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            # Use the [CLS] token embedding from last hidden state\n",
    "            embedding = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding.squeeze())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "'''\n",
    "\n",
    "# Write utility functions to file\n",
    "with open('../utils/bert_utils.py', 'w') as f:\n",
    "    f.write(bert_utils_code)\n",
    "\n",
    "print(\"✅ BERT utilities saved to ../utils/bert_utils.py\")\n",
    "print(\"🎯 Other team members can now import and use BERT functions!\")\n",
    "\n",
    "print(f\"\\n🏁 MEMBER 3 BERT DEVELOPMENT SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"✅ BERT model trained and saved\")\n",
    "print(f\"✅ Test accuracy achieved: {bert_accuracy:.3f}\")\n",
    "print(f\"✅ Model comparison with TF-IDF completed\")\n",
    "print(f\"✅ Error analysis and visualizations created\")\n",
    "print(f\"✅ Utility functions created for team integration\")\n",
    "print(f\"✅ All files saved for Member 4 (hybrid) and Member 5 (app)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7989ca-cc52-49b0-8e30-a0233b126ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
