{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a567b15-2b85-4e93-9287-1a3ab424298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting BERT Model Development...\n",
      "üíª Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"3\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Starting BERT Model Development...\")\n",
    "print(f\"üíª Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24ead004-dba9-49b6-8127-f8eec4844819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading cleaned data from Member 1...\n",
      "‚úÖ Data loaded successfully!\n",
      "Training: 10240 samples\n",
      "Test: 1267 samples\n",
      "Validation: 1284 samples\n",
      "\n",
      "‚ö° Using subset for faster training:\n",
      "   Training subset: 2000 samples\n",
      "   Test subset: 400 samples\n",
      "‚úÖ Sample created - Train: 2000, Test: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading cleaned data from Member 1...\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train_clean.csv')\n",
    "    test_df = pd.read_csv('../data/test_clean.csv')\n",
    "    valid_df = pd.read_csv('../data/valid_clean.csv')\n",
    "    \n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Training: {len(train_df)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    print(f\"Validation: {len(valid_df)} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Cleaned data not found!\")\n",
    "    print(\"Please run Member 1's notebook first (01_data_cleaning.ipynb)\")\n",
    "    exit()\n",
    "\n",
    "# For demonstration and speed, we'll use a subset of data\n",
    "# In production, you can use the full dataset\n",
    "SAMPLE_SIZE = 2000  # Adjust based on your computer's capability\n",
    "TEST_SIZE = 400\n",
    "\n",
    "print(f\"\\n‚ö° Using subset for faster training:\")\n",
    "print(f\"   Training subset: {SAMPLE_SIZE} samples\")\n",
    "print(f\"   Test subset: {TEST_SIZE} samples\")\n",
    "\n",
    "# Sample data (stratified to maintain label balance)\n",
    "train_sample = train_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLE_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_sample = test_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), TEST_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Sample created - Train: {len(train_sample)}, Test: {len(test_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56ec994-a5ac-4852-a402-af920a77e945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ INITIALIZING BERT MODEL...\n",
      "üì• Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BERT model initialized successfully!\n",
      "   Model: distilbert-base-uncased\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ INITIALIZING BERT MODEL...\")\n",
    "\n",
    "# Load pre-trained DistilBERT (faster than full BERT)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "print(\"üì• Loading tokenizer and model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ BERT model initialized successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec27968-d776-4328-830d-e43ba60306c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
