{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a567b15-2b85-4e93-9287-1a3ab424298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting BERT Model Development...\n",
      "üíª Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"3\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Starting BERT Model Development...\")\n",
    "print(f\"üíª Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ead004-dba9-49b6-8127-f8eec4844819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading cleaned data from Member 1...\n",
      "‚úÖ Data loaded successfully!\n",
      "Training: 10240 samples\n",
      "Test: 1267 samples\n",
      "Validation: 1284 samples\n",
      "\n",
      "‚ö° Using subset for faster training:\n",
      "   Training subset: 2000 samples\n",
      "   Test subset: 400 samples\n",
      "‚úÖ Sample created - Train: 2000, Test: 400\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading cleaned data from Member 1...\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train_clean.csv')\n",
    "    test_df = pd.read_csv('../data/test_clean.csv')\n",
    "    valid_df = pd.read_csv('../data/valid_clean.csv')\n",
    "    \n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Training: {len(train_df)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    print(f\"Validation: {len(valid_df)} samples\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Cleaned data not found!\")\n",
    "    print(\"Please run Member 1's notebook first (01_data_cleaning.ipynb)\")\n",
    "    exit()\n",
    "\n",
    "# For demonstration and speed, we'll use a subset of data\n",
    "# In production, you can use the full dataset\n",
    "SAMPLE_SIZE = 2000  # Adjust based on your computer's capability\n",
    "TEST_SIZE = 400\n",
    "\n",
    "print(f\"\\n‚ö° Using subset for faster training:\")\n",
    "print(f\"   Training subset: {SAMPLE_SIZE} samples\")\n",
    "print(f\"   Test subset: {TEST_SIZE} samples\")\n",
    "\n",
    "# Sample data (stratified to maintain label balance)\n",
    "train_sample = train_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLE_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_sample = test_df.groupby('label_binary').apply(\n",
    "    lambda x: x.sample(min(len(x), TEST_SIZE//2), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Sample created - Train: {len(train_sample)}, Test: {len(test_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56ec994-a5ac-4852-a402-af920a77e945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ INITIALIZING BERT MODEL...\n",
      "üì• Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BERT model initialized successfully!\n",
      "   Model: distilbert-base-uncased\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ INITIALIZING BERT MODEL...\")\n",
    "\n",
    "# Load pre-trained DistilBERT (faster than full BERT)\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "print(\"üì• Loading tokenizer and model...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ BERT model initialized successfully!\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec27968-d776-4328-830d-e43ba60306c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ TOKENIZING TEXT DATA...\n",
      "  üîÑ Tokenizing 2000 texts...\n",
      "  üîÑ Tokenizing 400 texts...\n",
      "‚úÖ Tokenization complete!\n",
      "   Max sequence length: 128\n",
      "   Training tokens shape: torch.Size([2000, 66])\n",
      "   Test tokens shape: torch.Size([400, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüî§ TOKENIZING TEXT DATA...\")\n",
    "\n",
    "def tokenize_data(texts, labels, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize text data for BERT input\n",
    "    \"\"\"\n",
    "    print(f\"  üîÑ Tokenizing {len(texts)} texts...\")\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return encodings, torch.tensor(labels.values)\n",
    "\n",
    "# Tokenize training data\n",
    "train_texts = train_sample['clean_statement'].fillna('')\n",
    "train_labels = train_sample['label_binary']\n",
    "\n",
    "train_encodings, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
    "\n",
    "# Tokenize test data  \n",
    "test_texts = test_sample['clean_statement'].fillna('')\n",
    "test_labels = test_sample['label_binary']\n",
    "\n",
    "test_encodings, test_labels_tensor = tokenize_data(test_texts, test_labels)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "print(f\"   Max sequence length: 128\")\n",
    "print(f\"   Training tokens shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"   Test tokens shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d64d32-a1ff-4757-8b92-0c9414e808e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Creating PyTorch datasets...\n",
      "‚úÖ Datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for BERT training\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "print(\"üì¶ Creating PyTorch datasets...\")\n",
    "train_dataset = NewsDataset(train_encodings, train_labels_tensor)\n",
    "test_dataset = NewsDataset(test_encodings, test_labels_tensor)\n",
    "\n",
    "print(\"‚úÖ Datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6117b3f0-c159-4e17-9af5-05374474831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öô CONFIGURING TRAINING PARAMETERS...\n",
      "‚úÖ Training configuration set!\n",
      "   Epochs: 3\n",
      "   Batch size: 8\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öô CONFIGURING TRAINING PARAMETERS...\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../models/saved_bert_model', exist_ok=True)\n",
    "\n",
    "# Training arguments - UPDATED for newer Transformers version\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/bert_results',\n",
    "    num_train_epochs=3,                 # Number of training epochs\n",
    "    per_device_train_batch_size=8,      # Batch size (reduce if out of memory)\n",
    "    per_device_eval_batch_size=16,      # Evaluation batch size\n",
    "    warmup_steps=100,                   # Warmup steps for learning rate\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    logging_dir='../models/bert_logs',  # Directory for storing logs\n",
    "    logging_steps=50,                   # Log every 50 steps\n",
    "    eval_strategy=\"steps\",              # CHANGED: evaluation_strategy ‚Üí eval_strategy\n",
    "    eval_steps=100,                     # Evaluation frequency\n",
    "    save_strategy=\"steps\",              # Save model every save_steps\n",
    "    save_steps=200,                     # Save frequency\n",
    "    load_best_model_at_end=True,        # Load best model at end\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,                     # Disable wandb logging\n",
    "    save_total_limit=2,                 # Keep only 2 best models\n",
    "    seed=42                             # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4781ff4-64b3-4a10-8444-00f5bacfadf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ INITIALIZING TRAINER...\n",
      "‚úÖ Trainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    if not hasattr(tf, 'random'):\n",
    "        import tensorflow._api.v2.random as tf_random\n",
    "        tf.random = tf_random\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"\\nüéØ INITIALIZING TRAINER...\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7221ac83-00e1-4c7c-84d9-234b6fc92f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING BERT TRAINING...\n",
      "‚è∞ This may take 10-30 minutes depending on your hardware...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 19:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.686828</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.385580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.681283</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.570771</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.562443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.664400</td>\n",
       "      <td>0.681836</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.573472</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.571082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.689164</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.598807</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.596165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.702249</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.610397</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.609649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.882805</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.902345</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.597437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed successfully!\n",
      "   Final training loss: 0.5616\n",
      "\n",
      "üìä Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation complete!\n",
      "   Loss: 0.883\n",
      "   Accuracy: 0.620\n",
      "   Precision: 0.625\n",
      "   Recall: 0.620\n",
      "   F1: 0.616\n",
      "   Runtime: 27.966\n",
      "   Samples_Per_Second: 14.303\n",
      "   Steps_Per_Second: 0.894\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ STARTING BERT TRAINING...\")\n",
    "print(\"‚è∞ This may take 10-30 minutes depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    training_results = trainer.train()\n",
    "    \n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"   Final training loss: {training_results.training_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüìä Evaluating on test set...\")\n",
    "    eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation complete!\")\n",
    "    for metric, value in eval_results.items():\n",
    "        if 'eval_' in metric:\n",
    "            print(f\"   {metric.replace('eval_', '').title()}: {value:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    print(\"üí° Try reducing batch size or sample size if out of memory\")\n",
    "    print(\"   You can also use CPU-only training by setting device='cpu'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd24651-b3b4-4c8d-aad8-ecdad84513b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ SAVING BERT MODEL...\n",
      "‚úÖ BERT model saved successfully!\n",
      "üìÅ Files saved:\n",
      "   - ../models/saved_bert_model/ (directory)\n",
      "   - Model config, weights, and tokenizer\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüíæ SAVING BERT MODEL...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained('../models/saved_bert_model')\n",
    "tokenizer.save_pretrained('../models/saved_bert_model')\n",
    "\n",
    "print(\"‚úÖ BERT model saved successfully!\")\n",
    "print(\"üìÅ Files saved:\")\n",
    "print(\"   - ../models/saved_bert_model/ (directory)\")\n",
    "print(\"   - Model config, weights, and tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce86eb24-a9b7-41b5-90f3-c0e1aec8ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ TESTING BERT MODEL...\n",
      "\n",
      "üß™ TESTING BERT WITH SAMPLE STATEMENTS:\n",
      "======================================================================\n",
      "\n",
      "üì∞ Test 1:\n",
      "   Statement: The President announced new infrastructure spending today.\n",
      "   ü§ñ BERT Prediction: Fake\n",
      "   üìä Confidence: 0.809\n",
      "   üî¥ Fake probability: 0.809\n",
      "   üü¢ Real probability: 0.191\n",
      "\n",
      "üì∞ Test 2:\n",
      "   Statement: Scientists have proven that the Earth is actually flat!\n",
      "   ü§ñ BERT Prediction: Fake\n",
      "   üìä Confidence: 0.931\n",
      "   üî¥ Fake probability: 0.931\n",
      "   üü¢ Real probability: 0.069\n",
      "\n",
      "üì∞ Test 3:\n",
      "   Statement: Stock markets reached record highs following positive earnings reports.\n",
      "   ü§ñ BERT Prediction: Real\n",
      "   üìä Confidence: 0.939\n",
      "   üî¥ Fake probability: 0.061\n",
      "   üü¢ Real probability: 0.939\n",
      "\n",
      "üì∞ Test 4:\n",
      "   Statement: Breaking: Aliens have invaded Earth and are demanding pizza!\n",
      "   ü§ñ BERT Prediction: Fake\n",
      "   üìä Confidence: 0.951\n",
      "   üî¥ Fake probability: 0.951\n",
      "   üü¢ Real probability: 0.049\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüß™ TESTING BERT MODEL...\")\n",
    "\n",
    "def predict_with_bert(text, model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Make prediction using trained BERT model\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Loading saved model...\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('../models/saved_bert_model')\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('../models/saved_bert_model')\n",
    "        model.to(device)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions.max().item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'Real' if predicted_class == 1 else 'Fake',\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'Fake': predictions[0][0].item(),\n",
    "            'Real': predictions[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test with sample statements\n",
    "test_statements = [\n",
    "    \"The President announced new infrastructure spending today.\",\n",
    "    \"Scientists have proven that the Earth is actually flat!\",\n",
    "    \"Stock markets reached record highs following positive earnings reports.\",\n",
    "    \"Breaking: Aliens have invaded Earth and are demanding pizza!\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ TESTING BERT WITH SAMPLE STATEMENTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, statement in enumerate(test_statements, 1):\n",
    "    try:\n",
    "        result = predict_with_bert(statement, model, tokenizer)\n",
    "        print(f\"\\nüì∞ Test {i}:\")\n",
    "        print(f\"   Statement: {statement}\")\n",
    "        print(f\"   ü§ñ BERT Prediction: {result['prediction']}\")\n",
    "        print(f\"   üìä Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"   üî¥ Fake probability: {result['probabilities']['Fake']:.3f}\")\n",
    "        print(f\"   üü¢ Real probability: {result['probabilities']['Real']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing statement {i}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb67c8-df7a-4b5a-b631-b6375ebc0cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
