{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339f539d-56f1-4eba-93e3-84af2a129c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Hybrid Model Development...\n",
      " Combining TF-IDF + Metadata + BERT features\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../utils')\n",
    "\n",
    "print(\" Starting Hybrid Model Development...\")\n",
    "print(\" Combining TF-IDF + Metadata + BERT features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b16c764-b369-4c63-aa1e-4a384d410cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and pre-trained models...\n",
      "Data loaded successfully!\n",
      "TF-IDF model loaded from Member 2!\n",
      "BERT model found from Member 3!\n",
      "BERT model loaded successfully!\n",
      "\n",
      "Dataset sizes:\n",
      "  Training: 10240\n",
      "  Test: 1267\n",
      "  Validation: 1284\n"
     ]
    }
   ],
   "source": [
    "#Load All Data and Models\n",
    "print(\"Loading data and pre-trained models...\")\n",
    "\n",
    "try:\n",
    "    # Load cleaned data\n",
    "    train_df = pd.read_csv('../data/train_clean.csv')\n",
    "    test_df = pd.read_csv('../data/test_clean.csv')\n",
    "    valid_df = pd.read_csv('../data/valid_clean.csv')\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    \n",
    "    # Load TF-IDF components from Member 2\n",
    "    tfidf_model = joblib.load('../models/tfidf_model.pkl')\n",
    "    tfidf_vectorizer = joblib.load('../models/tfidf_vectorizer.pkl')\n",
    "    \n",
    "    print(\"TF-IDF model loaded from Member 2!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Required files not found: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. Member 1 has run data cleaning notebook\")\n",
    "    print(\"2. Member 2 has run TF-IDF model notebook\")\n",
    "    exit()\n",
    "\n",
    "# Check if BERT model is available\n",
    "bert_available = os.path.exists('../models/saved_bert_model')\n",
    "if bert_available:\n",
    "    print(\"BERT model found from Member 3!\")\n",
    "    try:\n",
    "        sys.path.append('../utils')\n",
    "        from bert_utils import get_bert_embeddings, load_bert_model\n",
    "        bert_model, bert_tokenizer, device = load_bert_model()\n",
    "        print(\"BERT model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  BERT loading error: {e}\")\n",
    "        bert_available = False\n",
    "else:\n",
    "    print(\"âš  BERT model not found. Using TF-IDF + Metadata only.\")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_df)}\")\n",
    "print(f\"  Test: {len(test_df)}\")\n",
    "print(f\"  Validation: {len(valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea7859a-1382-4761-a1f7-52cff56d25d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXTRACTING TEXT FEATURES...\n",
      " TF-IDF features extracted:\n",
      "   Dimensions: 5000 features\n",
      "   Training samples: 10240\n",
      "\n",
      " Extracting BERT embeddings...\n",
      " BERT embeddings extracted:\n",
      " Embedding size: 768 dimensions\n",
      " Training samples: 1000\n",
      " Test samples: 200\n"
     ]
    }
   ],
   "source": [
    "#Feature Engineering - Text Features\n",
    "print(\"\\n EXTRACTING TEXT FEATURES...\")\n",
    "\n",
    "# Get TF-IDF features for all datasets\n",
    "X_train_tfidf = tfidf_vectorizer.transform(train_df['clean_statement'].fillna(''))\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['clean_statement'].fillna(''))\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid_df['clean_statement'].fillna(''))\n",
    "\n",
    "print(f\" TF-IDF features extracted:\")\n",
    "print(f\"   Dimensions: {X_train_tfidf.shape[1]} features\")\n",
    "print(f\"   Training samples: {X_train_tfidf.shape[0]}\")\n",
    "\n",
    "# Get BERT embeddings if available (using smaller sample for speed)\n",
    "if bert_available:\n",
    "    print(\"\\n Extracting BERT embeddings...\")\n",
    "    \n",
    "    # Use sample for speed (adjust BERT_SAMPLE_SIZE based on your computer)\n",
    "    BERT_SAMPLE_SIZE = min(1000, len(train_df))\n",
    "    \n",
    "    train_sample_bert = train_df.sample(n=BERT_SAMPLE_SIZE, random_state=42)\n",
    "    test_sample_bert = test_df.sample(n=min(200, len(test_df)), random_state=42)\n",
    "    \n",
    "    try:\n",
    "        train_bert_embeddings = get_bert_embeddings(train_sample_bert['clean_statement'].fillna(''))\n",
    "        test_bert_embeddings = get_bert_embeddings(test_sample_bert['clean_statement'].fillna(''))\n",
    "        \n",
    "        print(f\" BERT embeddings extracted:\")\n",
    "        print(f\" Embedding size: {train_bert_embeddings.shape[1]} dimensions\")\n",
    "        print(f\" Training samples: {train_bert_embeddings.shape[0]}\")\n",
    "        print(f\" Test samples: {test_bert_embeddings.shape[0]}\")\n",
    "        \n",
    "        # Update dataframes to match BERT sample\n",
    "        train_df_bert = train_sample_bert.reset_index(drop=True)\n",
    "        test_df_bert = test_sample_bert.reset_index(drop=True)\n",
    "        \n",
    "        # Update TF-IDF features to match BERT sample\n",
    "        X_train_tfidf_bert = tfidf_vectorizer.transform(train_df_bert['clean_statement'].fillna(''))\n",
    "        X_test_tfidf_bert = tfidf_vectorizer.transform(test_df_bert['clean_statement'].fillna(''))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" BERT embedding extraction failed: {e}\")\n",
    "        bert_available = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ce6a6d-9ffe-4e19-af21-6697b3474615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXTRACTING METADATA FEATURES...\n",
      " Metadata features extracted:\n",
      " Features: ['party_code', 'subject_code', 'speaker_code', 'job_code', 'state_code', 'statement_length', 'word_count', 'has_question', 'has_exclamation', 'has_quotes', 'uppercase_ratio', 'avg_word_length']\n",
      " Dimensions: 12 features\n"
     ]
    }
   ],
   "source": [
    "#Feature Engineering - Metadata Features\n",
    "print(\"\\n EXTRACTING METADATA FEATURES...\")\n",
    "\n",
    "def extract_metadata_features(df):\n",
    "    \"\"\"Extract and engineer metadata features\"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Basic categorical features (using codes created by Member 1)\n",
    "    categorical_features = ['party_code', 'subject_code', 'speaker_code', 'job_code', 'state_code']\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            features.append(df[feature].fillna(-1).values.reshape(-1, 1))\n",
    "            feature_names.append(feature)\n",
    "    \n",
    "    # Text-based numerical features\n",
    "    text_features = ['statement_length', 'word_count', 'has_question', 'has_exclamation', \n",
    "                    'has_quotes', 'uppercase_ratio', 'avg_word_length']\n",
    "    \n",
    "    for feature in text_features:\n",
    "        if feature in df.columns:\n",
    "            features.append(df[feature].fillna(0).values.reshape(-1, 1))\n",
    "            feature_names.append(feature)\n",
    "        else:\n",
    "            # Create feature if missing\n",
    "            if feature == 'statement_length':\n",
    "                values = df['clean_statement'].fillna('').str.len().values.reshape(-1, 1)\n",
    "            elif feature == 'word_count':\n",
    "                values = df['clean_statement'].fillna('').str.split().str.len().fillna(0).values.reshape(-1, 1)\n",
    "            elif feature == 'has_question':\n",
    "                values = df['statement'].fillna('').str.contains('\\?').astype(int).values.reshape(-1, 1)\n",
    "            elif feature == 'has_exclamation':\n",
    "                values = df['statement'].fillna('').str.contains('!').astype(int).values.reshape(-1, 1)\n",
    "            elif feature == 'has_quotes':\n",
    "                values = df['statement'].fillna('').str.contains('\"').astype(int).values.reshape(-1, 1)\n",
    "            elif feature == 'uppercase_ratio':\n",
    "                values = df['statement'].fillna('').apply(\n",
    "                    lambda x: sum(c.isupper() for c in str(x)) / max(len(str(x)), 1)\n",
    "                ).values.reshape(-1, 1)\n",
    "            elif feature == 'avg_word_length':\n",
    "                values = df['clean_statement'].fillna('').apply(\n",
    "                    lambda x: np.mean([len(word) for word in str(x).split()]) if str(x).split() else 0\n",
    "                ).values.reshape(-1, 1)\n",
    "            else:\n",
    "                values = np.zeros((len(df), 1))\n",
    "            \n",
    "            features.append(values)\n",
    "            feature_names.append(feature)\n",
    "    \n",
    "    # Combine all features\n",
    "    if features:\n",
    "        combined_features = np.hstack(features)\n",
    "    else:\n",
    "        combined_features = np.zeros((len(df), 1))\n",
    "    \n",
    "    return combined_features, feature_names\n",
    "\n",
    "# Extract metadata features\n",
    "train_meta_features, meta_feature_names = extract_metadata_features(train_df)\n",
    "test_meta_features, _ = extract_metadata_features(test_df)\n",
    "valid_meta_features, _ = extract_metadata_features(valid_df)\n",
    "\n",
    "print(f\" Metadata features extracted:\")\n",
    "print(f\" Features: {meta_feature_names}\")\n",
    "print(f\" Dimensions: {train_meta_features.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ad7b91-08db-4485-bca6-8cb21cfa5534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CREATING HYBRID FEATURE COMBINATIONS...\n",
      " Feature combinations created:\n",
      "   TF-IDF Only: 5000 features\n",
      "   Metadata Only: 2 features\n",
      "   TF-IDF + Metadata: 5002 features\n",
      "   BERT Only: 768 features\n",
      "   BERT + Metadata: 770 features\n",
      "   TF-IDF + Metadata + BERT: 5770 features\n"
     ]
    }
   ],
   "source": [
    "#Create Hybrid Feature Combinations\n",
    "print(\"\\n CREATING HYBRID FEATURE COMBINATIONS...\")\n",
    "\n",
    "def create_hybrid_features(tfidf_features, meta_features, bert_embeddings=None, feature_names=None):\n",
    "    \"\"\"Combine different types of features\"\"\"\n",
    "    \n",
    "    # Start with TF-IDF features (convert sparse to dense)\n",
    "    if hasattr(tfidf_features, 'toarray'):\n",
    "        tfidf_dense = tfidf_features.toarray()\n",
    "    else:\n",
    "        tfidf_dense = tfidf_features\n",
    "    \n",
    "    # Combine TF-IDF + Metadata\n",
    "    tfidf_meta_features = np.hstack([tfidf_dense, meta_features])\n",
    "    \n",
    "    feature_combinations = {\n",
    "        'tfidf_only': tfidf_dense,\n",
    "        'metadata_only': meta_features,\n",
    "        'tfidf_metadata': tfidf_meta_features\n",
    "    }\n",
    "    \n",
    "    combination_names = {\n",
    "        'tfidf_only': 'TF-IDF Only',\n",
    "        'metadata_only': 'Metadata Only', \n",
    "        'tfidf_metadata': 'TF-IDF + Metadata'\n",
    "    }\n",
    "    \n",
    "    # Add BERT combinations if available\n",
    "    if bert_embeddings is not None:\n",
    "        # BERT only\n",
    "        feature_combinations['bert_only'] = bert_embeddings\n",
    "        combination_names['bert_only'] = 'BERT Only'\n",
    "        \n",
    "        # BERT + Metadata (ensure same number of samples)\n",
    "        min_samples = min(len(bert_embeddings), len(meta_features))\n",
    "        bert_meta_features = np.hstack([\n",
    "            bert_embeddings[:min_samples], \n",
    "            meta_features[:min_samples]\n",
    "        ])\n",
    "        feature_combinations['bert_metadata'] = bert_meta_features\n",
    "        combination_names['bert_metadata'] = 'BERT + Metadata'\n",
    "        \n",
    "        # All features combined\n",
    "        tfidf_subset = tfidf_dense[:min_samples]\n",
    "        all_features = np.hstack([\n",
    "            tfidf_subset, \n",
    "            meta_features[:min_samples], \n",
    "            bert_embeddings[:min_samples]\n",
    "        ])\n",
    "        feature_combinations['all_features'] = all_features\n",
    "        combination_names['all_features'] = 'TF-IDF + Metadata + BERT'\n",
    "    \n",
    "    return feature_combinations, combination_names\n",
    "\n",
    "# Create feature combinations\n",
    "if bert_available:\n",
    "    train_combinations, combination_names = create_hybrid_features(\n",
    "        X_train_tfidf_bert, train_df_bert[['party_code', 'subject_code']].fillna(-1).values, \n",
    "        train_bert_embeddings\n",
    "    )\n",
    "    test_combinations, _ = create_hybrid_features(\n",
    "        X_test_tfidf_bert, test_df_bert[['party_code', 'subject_code']].fillna(-1).values,\n",
    "        test_bert_embeddings\n",
    "    )\n",
    "else:\n",
    "    train_combinations, combination_names = create_hybrid_features(\n",
    "        X_train_tfidf, train_meta_features\n",
    "    )\n",
    "    test_combinations, _ = create_hybrid_features(\n",
    "        X_test_tfidf, test_meta_features\n",
    "    )\n",
    "\n",
    "print(f\" Feature combinations created:\")\n",
    "for combo_name, combo_desc in combination_names.items():\n",
    "    print(f\"   {combo_desc}: {train_combinations[combo_name].shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff588c-1d0d-4287-b440-735d8b9105cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ TRAINING HYBRID MODELS...\n",
      "Target distribution - Real: 574, Fake: 426\n",
      "\n",
      " Training models with TF-IDF Only...\n",
      " Training logistic_regression...\n",
      " logistic_regression: 0.625 accuracy\n",
      " Training random_forest...\n",
      " random_forest: 0.600 accuracy\n",
      " Training gradient_boosting...\n",
      " gradient_boosting: 0.590 accuracy\n",
      " Training svm...\n",
      " svm: 0.600 accuracy\n",
      "\n",
      " Training models with Metadata Only...\n",
      " Training logistic_regression...\n",
      " logistic_regression: 0.570 accuracy\n",
      " Training random_forest...\n",
      " random_forest: 0.440 accuracy\n",
      " Training gradient_boosting...\n",
      " gradient_boosting: 0.440 accuracy\n",
      " Training svm...\n",
      " svm: 0.570 accuracy\n",
      "\n",
      " Training models with TF-IDF + Metadata...\n",
      " Training logistic_regression...\n",
      " logistic_regression: 0.620 accuracy\n",
      " Training random_forest...\n",
      " random_forest: 0.605 accuracy\n",
      " Training gradient_boosting...\n",
      " gradient_boosting: 0.580 accuracy\n",
      " Training svm...\n"
     ]
    }
   ],
   "source": [
    "#Train Multiple Hybrid Models\n",
    "print(\"\\nðŸŽ¯ TRAINING HYBRID MODELS...\")\n",
    "\n",
    "# Define models to test\n",
    "model_configs = {\n",
    "    'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'svm': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Get target variables\n",
    "if bert_available:\n",
    "    y_train = train_df_bert['label_binary'].values\n",
    "    y_test = test_df_bert['label_binary'].values\n",
    "else:\n",
    "    y_train = train_df['label_binary'].values\n",
    "    y_test = test_df['label_binary'].values\n",
    "\n",
    "print(f\"Target distribution - Real: {sum(y_train == 1)}, Fake: {sum(y_train == 0)}\")\n",
    "\n",
    "# Train models for each feature combination\n",
    "for combo_name, combo_desc in combination_names.items():\n",
    "    print(f\"\\n Training models with {combo_desc}...\")\n",
    "    \n",
    "    X_train_combo = train_combinations[combo_name]\n",
    "    X_test_combo = test_combinations[combo_name]\n",
    "    \n",
    "    # Scale features for better performance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_combo)\n",
    "    X_test_scaled = scaler.transform(X_test_combo)\n",
    "    \n",
    "    combo_results = {}\n",
    "    \n",
    "    for model_name, model in model_configs.items():\n",
    "        try:\n",
    "            print(f\" Training {model_name}...\")\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            train_pred = model.predict(X_train_scaled)\n",
    "            test_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            # Store results\n",
    "            combo_results[model_name] = {\n",
    "                'model': model,\n",
    "                'scaler': scaler,\n",
    "                'train_accuracy': train_acc,\n",
    "                'test_accuracy': test_acc,\n",
    "                'predictions': test_pred,\n",
    "                'feature_count': X_train_combo.shape[1]\n",
    "            }\n",
    "            \n",
    "            print(f\" {model_name}: {test_acc:.3f} accuracy\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" {model_name} failed: {str(e)}\")\n",
    "    \n",
    "    all_results[combo_name] = combo_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3221d-6118-48d8-873d-dd80daba3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Best Model Combination\n",
    "print(\"\\n FINDING BEST MODEL COMBINATION...\")\n",
    "\n",
    "best_accuracy = 0\n",
    "best_combination = None\n",
    "best_model_name = None\n",
    "\n",
    "# Create comprehensive results table\n",
    "results_table = []\n",
    "\n",
    "for combo_name, combo_results in all_results.items():\n",
    "    for model_name, model_results in combo_results.items():\n",
    "        test_acc = model_results['test_accuracy']\n",
    "        \n",
    "        results_table.append({\n",
    "            'Feature_Combination': combination_names[combo_name],\n",
    "            'Model': model_name.replace('_', ' ').title(),\n",
    "            'Test_Accuracy': test_acc,\n",
    "            'Train_Accuracy': model_results['train_accuracy'],\n",
    "            'Overfitting': model_results['train_accuracy'] - test_acc,\n",
    "            'Feature_Count': model_results['feature_count']\n",
    "        })\n",
    "        \n",
    "        # Track best model\n",
    "        if test_acc > best_accuracy:\n",
    "            best_accuracy = test_acc\n",
    "            best_combination = combo_name\n",
    "            best_model_name = model_name\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "results_df = pd.DataFrame(results_table)\n",
    "results_df = results_df.sort_values('Test_Accuracy', ascending=False)\n",
    "\n",
    "print(\" COMPREHENSIVE RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n BEST MODEL:\")\n",
    "print(f\"   Combination: {combination_names[best_combination]}\")\n",
    "print(f\"   Algorithm: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"   Test Accuracy: {best_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ed1f6-dafc-423d-a4de-b0cbbf95a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detailed Analysis of Best Model\n",
    "print(f\"\\n DETAILED ANALYSIS OF BEST MODEL...\")\n",
    "\n",
    "best_model_obj = all_results[best_combination][best_model_name]\n",
    "best_model = best_model_obj['model']\n",
    "best_scaler = best_model_obj['scaler']\n",
    "best_predictions = best_model_obj['predictions']\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, best_predictions, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title(f'Best Model Confusion Matrix\\n{combination_names[best_combination]} + {best_model_name.title()}')\n",
    "\n",
    "# Feature importance (if applicable)\n",
    "plt.subplot(2, 3, 2)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    top_indices = importances.argsort()[-20:]\n",
    "    \n",
    "    plt.barh(range(len(top_indices)), importances[top_indices])\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    coef = best_model.coef_[0]\n",
    "    top_positive = coef.argsort()[-10:]\n",
    "    top_negative = coef.argsort()[:10]\n",
    "    \n",
    "    all_indices = np.concatenate([top_negative, top_positive])\n",
    "    plt.barh(range(len(all_indices)), coef[all_indices])\n",
    "    plt.title('Top Positive/Negative Coefficients')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "\n",
    "# Accuracy comparison across feature combinations\n",
    "plt.subplot(2, 3, 3)\n",
    "combo_accs = {}\n",
    "for combo_name in combination_names.keys():\n",
    "    if combo_name in all_results:\n",
    "        best_acc_for_combo = max([r['test_accuracy'] for r in all_results[combo_name].values()])\n",
    "        combo_accs[combination_names[combo_name]] = best_acc_for_combo\n",
    "\n",
    "combo_names = list(combo_accs.keys())\n",
    "combo_values = list(combo_accs.values())\n",
    "\n",
    "plt.bar(range(len(combo_names)), combo_values, color='lightblue')\n",
    "plt.xticks(range(len(combo_names)), combo_names, rotation=45, ha='right')\n",
    "plt.title('Best Accuracy by Feature Combination')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Model comparison for best feature combination\n",
    "plt.subplot(2, 3, 4)\n",
    "if best_combination in all_results:\n",
    "    model_accs = {name.replace('_', ' ').title(): results['test_accuracy'] \n",
    "                 for name, results in all_results[best_combination].items()}\n",
    "    \n",
    "    plt.bar(model_accs.keys(), model_accs.values(), color='lightgreen')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f'Model Comparison\\n{combination_names[best_combination]}')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "# Cross-validation analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "try:\n",
    "    cv_scores = cross_val_score(\n",
    "        best_model, \n",
    "        best_scaler.transform(train_combinations[best_combination]), \n",
    "        y_train, \n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    plt.boxplot([cv_scores])\n",
    "    plt.scatter([1], [best_accuracy], color='red', s=100, label='Test Accuracy')\n",
    "    plt.xticks([1], ['CV Scores'])\n",
    "    plt.title(f'Cross-Validation Analysis\\nMean: {cv_scores.mean():.3f}Â±{cv_scores.std():.3f}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "except Exception as e:\n",
    "    plt.text(0.5, 0.5, f'CV Error: {str(e)}', ha='center', va='center')\n",
    "    plt.title('Cross-Validation Failed')\n",
    "\n",
    "# Learning curve (for ensemble models)\n",
    "plt.subplot(2, 3, 6)\n",
    "if hasattr(best_model, 'staged_predict_proba'):\n",
    "    # For GradientBoosting, show staged predictions\n",
    "    staged_predictions = list(best_model.staged_predict(\n",
    "        best_scaler.transform(test_combinations[best_combination])\n",
    "    ))\n",
    "    staged_accuracies = [accuracy_score(y_test, pred) for pred in staged_predictions]\n",
    "    \n",
    "    plt.plot(range(1, len(staged_accuracies) + 1), staged_accuracies, 'b-o')\n",
    "    plt.title('Learning Curve (Gradient Boosting)')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Learning curve not\\navailable for this model', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    plt.title('Learning Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aca6e8-afb8-46fb-a60b-e06c18189830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization\n",
    "print(\"\\nâš™ HYPERPARAMETER OPTIMIZATION...\")\n",
    "\n",
    "# Optimize best model with GridSearchCV\n",
    "print(f\" Optimizing {best_model_name} with {combination_names[best_combination]}...\")\n",
    "\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    if best_model_name in param_grids:\n",
    "        param_grid = param_grids[best_model_name]\n",
    "        \n",
    "        # Create fresh model instance for GridSearch\n",
    "        base_model = model_configs[best_model_name]\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model,\n",
    "            param_grid,\n",
    "            cv=3,  # 3-fold CV for speed\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        X_train_best = best_scaler.transform(train_combinations[best_combination])\n",
    "        grid_search.fit(X_train_best, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        optimized_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Test optimized model\n",
    "        X_test_best = best_scaler.transform(test_combinations[best_combination])\n",
    "        optimized_predictions = optimized_model.predict(X_test_best)\n",
    "        optimized_accuracy = accuracy_score(y_test, optimized_predictions)\n",
    "        \n",
    "        print(f\" Hyperparameter optimization complete!\")\n",
    "        print(f\" Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   Original accuracy: {best_accuracy:.3f}\")\n",
    "        print(f\"   Optimized accuracy: {optimized_accuracy:.3f}\")\n",
    "        print(f\"   Improvement: {optimized_accuracy - best_accuracy:.3f}\")\n",
    "        \n",
    "        # Update best model if optimization improved it\n",
    "        if optimized_accuracy > best_accuracy:\n",
    "            best_model = optimized_model\n",
    "            best_accuracy = optimized_accuracy\n",
    "            best_predictions = optimized_predictions\n",
    "            print(\" Using optimized model as final model!\")\n",
    "        else:\n",
    "            print(\" Original model performs better, keeping original.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš  No parameter grid defined for {best_model_name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Hyperparameter optimization failed: {str(e)}\")\n",
    "    print(\" Using original model without optimization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1b0c3-107b-4611-a157-df593b3b51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance Analysis\n",
    "print(\"\\n FEATURE IMPORTANCE ANALYSIS...\")\n",
    "\n",
    "# Analyze feature importance for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\" Analyzing Random Forest/Gradient Boosting feature importance...\")\n",
    "    \n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Get feature names (approximate)\n",
    "    feature_names = []\n",
    "    \n",
    "    # TF-IDF features\n",
    "    if 'tfidf' in best_combination:\n",
    "        tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "        feature_names.extend([f'tfidf_{name}' for name in tfidf_features])\n",
    "    \n",
    "    # Metadata features\n",
    "    if 'metadata' in best_combination:\n",
    "        feature_names.extend([f'meta_{name}' for name in meta_feature_names])\n",
    "    \n",
    "    # BERT features\n",
    "    if 'bert' in best_combination and bert_available:\n",
    "        feature_names.extend([f'bert_dim_{i}' for i in range(train_bert_embeddings.shape[1])])\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    if len(feature_names) == len(importances):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Show top features\n",
    "        print(f\"\\nðŸ” Top 20 Most Important Features:\")\n",
    "        print(importance_df.head(20).to_string(index=False))\n",
    "        \n",
    "        # Save feature importance\n",
    "        importance_df.to_csv('../models/hybrid_feature_importance.csv', index=False)\n",
    "        print(\" Feature importance saved!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âš  Feature name mismatch: {len(feature_names)} names vs {len(importances)} features\")\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    print(\" Analyzing Logistic Regression coefficients...\")\n",
    "    \n",
    "    coef = best_model.coef_[0]\n",
    "    \n",
    "    # Top positive coefficients (indicate Real news)\n",
    "    top_real_indices = coef.argsort()[-10:]\n",
    "    print(f\"\\n Top features indicating REAL news:\")\n",
    "    for i, idx in enumerate(top_real_indices):\n",
    "        print(f\"   {i+1}. Feature {idx}: {coef[idx]:.3f}\")\n",
    "    \n",
    "    # Top negative coefficients (indicate Fake news)\n",
    "    top_fake_indices = coef.argsort()[:10]\n",
    "    print(f\"\\n Top features indicating FAKE news:\")\n",
    "    for i, idx in enumerate(top_fake_indices):\n",
    "        print(f\"   {i+1}. Feature {idx}: {coef[idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792915f-27a5-422c-b608-dbdac8249fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Hybrid Model\n",
    "print(\"\\n SAVING FINAL HYBRID MODEL...\")\n",
    "\n",
    "# Create final model package\n",
    "final_model_package = {\n",
    "    'model': best_model,\n",
    "    'scaler': best_scaler,\n",
    "    'feature_combination': best_combination,\n",
    "    'combination_name': combination_names[best_combination],\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': best_accuracy,\n",
    "    'feature_count': train_combinations[best_combination].shape[1],\n",
    "    'metadata_features': meta_feature_names\n",
    "}\n",
    "\n",
    "# Save final model\n",
    "joblib.dump(final_model_package, '../models/final_hybrid_model.pkl')\n",
    "\n",
    "# Also save individual components for Streamlit app\n",
    "joblib.dump(best_model, '../models/hybrid_model.pkl')\n",
    "joblib.dump(best_scaler, '../models/hybrid_scaler.pkl')\n",
    "\n",
    "# Save configuration for app\n",
    "model_config = {\n",
    "    'feature_combination': best_combination,\n",
    "    'combination_name': combination_names[best_combination],\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': best_accuracy,\n",
    "    'requires_bert': bert_available and 'bert' in best_combination,\n",
    "    'metadata_features': meta_feature_names\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/hybrid_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\" Final hybrid model saved!\")\n",
    "print(\" Files created:\")\n",
    "print(\"   - ../models/final_hybrid_model.pkl (complete package)\")\n",
    "print(\"   - ../models/hybrid_model.pkl (model only)\")\n",
    "print(\"   - ../models/hybrid_scaler.pkl (feature scaler)\")\n",
    "print(\"   - ../models/hybrid_config.json (configuration)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897347d-55b0-4db2-83fa-ee6bbcf8508f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Create Comprehensive Results Summary\n",
    "print(\"\\n CREATING COMPREHENSIVE RESULTS SUMMARY...\")\n",
    "\n",
    "# Save detailed results\n",
    "results_df.to_csv('../models/hybrid_model_results.csv', index=False)\n",
    "\n",
    "# Create summary for team\n",
    "team_summary = {\n",
    "    'Final Model Performance': {\n",
    "        'Best Feature Combination': combination_names[best_combination],\n",
    "        'Best Algorithm': best_model_name.replace('_', ' ').title(),\n",
    "        'Test Accuracy': f\"{best_accuracy:.3f}\",\n",
    "        'Feature Count': train_combinations[best_combination].shape[1],\n",
    "        'Uses BERT': bert_available and 'bert' in best_combination\n",
    "    },\n",
    "    'Model Comparisons': {\n",
    "        'TF-IDF Only': 'See detailed results table',\n",
    "        'Metadata Only': 'See detailed results table', \n",
    "        'TF-IDF + Metadata': 'See detailed results table',\n",
    "        'BERT Combinations': 'Available if BERT model exists'\n",
    "    },\n",
    "    'Files for Streamlit App': [\n",
    "        'hybrid_model.pkl',\n",
    "        'hybrid_scaler.pkl', \n",
    "        'hybrid_config.json',\n",
    "        'tfidf_vectorizer.pkl'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n TEAM SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "for section, content in team_summary.items():\n",
    "    print(f\"\\nðŸ“Œ {section}:\")\n",
    "    if isinstance(content, dict):\n",
    "        for key, value in content.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "    elif isinstance(content, list):\n",
    "        for item in content:\n",
    "            print(f\"   - {item}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97da941-bdc1-4c32-a68e-3866662286f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Testing Function for Streamlit Integration\n",
    "print(\"\\nðŸ›  CREATING TESTING FUNCTION FOR STREAMLIT...\")\n",
    "\n",
    "def predict_with_hybrid_model(statement, party='none', subject='none', \n",
    "                             model_path='../models/final_hybrid_model.pkl'):\n",
    "    \"\"\"\n",
    "    Predict using hybrid model - ready for Streamlit integration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model package\n",
    "        model_package = joblib.load(model_path)\n",
    "        \n",
    "        model = model_package['model']\n",
    "        scaler = model_package['scaler']\n",
    "        feature_combination = model_package['feature_combination']\n",
    "        \n",
    "        # Clean text\n",
    "        import re\n",
    "        clean_statement = re.sub(r'http\\S+', '', statement.lower())\n",
    "        clean_statement = re.sub(r'[^a-zA-Z\\s]', '', clean_statement)\n",
    "        clean_statement = re.sub(r'\\s+', ' ', clean_statement).strip()\n",
    "        \n",
    "        # Initialize feature list\n",
    "        features = []\n",
    "        \n",
    "        # TF-IDF features\n",
    "        if 'tfidf' in feature_combination:\n",
    "            tfidf_features = tfidf_vectorizer.transform([clean_statement])\n",
    "            features.append(tfidf_features.toarray())\n",
    "        \n",
    "        # Metadata features\n",
    "        if 'metadata' in feature_combination:\n",
    "            # Simple encoding for demo (in production, use proper label encoding)\n",
    "            party_map = {'republican': 0, 'democrat': 1, 'none': 2}\n",
    "            subject_map = {'politics': 0, 'healthcare': 1, 'education': 2, 'other': 3}\n",
    "            \n",
    "            party_code = party_map.get(party.lower(), 2)\n",
    "            subject_code = subject_map.get(subject.lower(), 3)\n",
    "            \n",
    "            # Create metadata feature vector\n",
    "            meta_features = np.array([[\n",
    "                party_code, subject_code, 0, 0, 0,  # Basic codes\n",
    "                len(clean_statement),  # statement_length\n",
    "                len(clean_statement.split()),  # word_count\n",
    "                1 if '?' in statement else 0,  # has_question\n",
    "                1 if '!' in statement else 0,  # has_exclamation\n",
    "                1 if '\"' in statement else 0,  # has_quotes\n",
    "                sum(c.isupper() for c in statement) / max(len(statement), 1),  # uppercase_ratio\n",
    "                np.mean([len(word) for word in clean_statement.split()]) if clean_statement.split() else 0  # avg_word_length\n",
    "            ]])\n",
    "            features.append(meta_features)\n",
    "        \n",
    "        # Combine features\n",
    "        if len(features) > 1:\n",
    "            combined_features = np.hstack(features)\n",
    "        else:\n",
    "            combined_features = features[0]\n",
    "        \n",
    "        # Scale features\n",
    "        scaled_features = scaler.transform(combined_features)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(scaled_features)[0]\n",
    "        probabilities = model.predict_proba(scaled_features)[0]\n",
    "        \n",
    "        return {\n",
    "            'prediction': 'Real' if prediction == 1 else 'Fake',\n",
    "            'confidence': max(probabilities),\n",
    "            'probabilities': {'Fake': probabilities[0], 'Real': probabilities[1]},\n",
    "            'feature_combination': model_package['combination_name']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Test the function\n",
    "print(\"\\n TESTING HYBRID MODEL FUNCTION:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        'statement': \"The President announced new economic policies today\",\n",
    "        'party': 'democrat',\n",
    "        'subject': 'politics'\n",
    "    },\n",
    "    {\n",
    "        'statement': \"Scientists have discovered that chocolate cures all diseases!\",\n",
    "        'party': 'none', \n",
    "        'subject': 'healthcare'\n",
    "    },\n",
    "    {\n",
    "        'statement': \"New education funding approved by Congress\",\n",
    "        'party': 'republican',\n",
    "        'subject': 'education'  \n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    result = predict_with_hybrid_model(\n",
    "        test_case['statement'],\n",
    "        test_case['party'], \n",
    "        test_case['subject']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Test {i}:\")\n",
    "    print(f\"   Statement: {test_case['statement']}\")\n",
    "    print(f\"   Party: {test_case['party']}, Subject: {test_case['subject']}\")\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"  Prediction: {result['prediction']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"  Features: {result['feature_combination']}\")\n",
    "    else:\n",
    "        print(f\"   Error: {result['error']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2903c-23a3-4bfe-926a-1038686f2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Model Validation\n",
    "print(\"\\n FINAL MODEL VALIDATION...\")\n",
    "\n",
    "# Cross-validation on best model\n",
    "try:\n",
    "    final_cv_scores = cross_val_score(\n",
    "        best_model,\n",
    "        best_scaler.transform(train_combinations[best_combination]),\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    print(f\" Final Model Cross-Validation Results:\")\n",
    "    print(f\"   Mean CV Accuracy: {final_cv_scores.mean():.3f}\")\n",
    "    print(f\"   CV Std Deviation: {final_cv_scores.std():.3f}\")\n",
    "    print(f\"   Test Accuracy: {best_accuracy:.3f}\")\n",
    "    print(f\"   CV vs Test Gap: {final_cv_scores.mean() - best_accuracy:.3f}\")\n",
    "    \n",
    "    if abs(final_cv_scores.mean() - best_accuracy) < 0.05:\n",
    "        print(\"    Model shows good generalization!\")\n",
    "    else:\n",
    "        print(\"    Potential overfitting detected\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš  Final validation error: {str(e)}\")\n",
    "\n",
    "# Performance summary table\n",
    "final_summary = pd.DataFrame({\n",
    "    'Metric': ['Test Accuracy', 'CV Mean', 'CV Std', 'Feature Count', 'Model Type'],\n",
    "    'Value': [\n",
    "        f\"{best_accuracy:.3f}\",\n",
    "        f\"{final_cv_scores.mean():.3f}\" if 'final_cv_scores' in locals() else \"N/A\",\n",
    "        f\"{final_cv_scores.std():.3f}\" if 'final_cv_scores' in locals() else \"N/A\", \n",
    "        train_combinations[best_combination].shape[1],\n",
    "        f\"{best_model_name.replace('_', ' ').title()}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\n INAL MODEL SUMMARY:\")\n",
    "print(final_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404d27b-d902-4e5c-a8a3-bb19c1bf21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Deployment Package\n",
    "print(\"\\n CREATING DEPLOYMENT PACKAGE...\")\n",
    "\n",
    "# Create deployment configuration\n",
    "deployment_config = {\n",
    "    'model_info': {\n",
    "        'name': 'Hybrid Fake News Detector',\n",
    "        'version': '1.0',\n",
    "        'feature_combination': combination_names[best_combination],\n",
    "        'algorithm': best_model_name.replace('_', ' ').title(),\n",
    "        'accuracy': best_accuracy,\n",
    "        'created_by': 'Member 4 - Hybrid Model Team'\n",
    "    },\n",
    "    'requirements': {\n",
    "        'tfidf_vectorizer': '../models/tfidf_vectorizer.pkl',\n",
    "        'hybrid_model': '../models/hybrid_model.pkl',\n",
    "        'hybrid_scaler': '../models/hybrid_scaler.pkl',\n",
    "        'bert_model': '../models/saved_bert_model/' if bert_available and 'bert' in best_combination else None\n",
    "    },\n",
    "    'metadata_encoding': {\n",
    "        'party_map': {'republican': 0, 'democrat': 1, 'none': 2},\n",
    "        'subject_map': {'politics': 0, 'healthcare': 1, 'education': 2, 'other': 3}\n",
    "    },\n",
    "    'usage_example': {\n",
    "        'statement': 'Sample news statement here',\n",
    "        'party': 'democrat',  \n",
    "        'subject': 'politics'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save deployment config\n",
    "with open('../models/deployment_config.json', 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "print(\" Deployment package created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21ac54-5695-47f7-a66f-a10fe4ec8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance Benchmark\n",
    "print(\"\\n FINAL PERFORMANCE BENCHMARK...\")\n",
    "\n",
    "# Compare all approaches\n",
    "benchmark_results = []\n",
    "\n",
    "# Individual model results\n",
    "for combo_name, combo_results in all_results.items():\n",
    "    for model_name, model_results in combo_results.items():\n",
    "        benchmark_results.append({\n",
    "            'Approach': f\"{combination_names[combo_name]} + {model_name.replace('_', ' ').title()}\",\n",
    "            'Test_Accuracy': model_results['test_accuracy'],\n",
    "            'Feature_Count': model_results['feature_count'],\n",
    "            'Complexity': 'Low' if model_results['feature_count'] < 100 else 'Medium' if model_results['feature_count'] < 1000 else 'High'\n",
    "        })\n",
    "\n",
    "# Create benchmark DataFrame\n",
    "benchmark_df = pd.DataFrame(benchmark_results)\n",
    "benchmark_df = benchmark_df.sort_values('Test_Accuracy', ascending=False)\n",
    "\n",
    "print(\" FINAL PERFORMANCE BENCHMARK:\")\n",
    "print(\"=\"*80)\n",
    "print(benchmark_df.head(10).to_string(index=False))\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark_df.to_csv('../models/performance_benchmark.csv', index=False)\n",
    "\n",
    "# Visualization of benchmark\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "top_10 = benchmark_df.head(10)\n",
    "plt.barh(range(len(top_10)), top_10['Test_Accuracy'], color='steelblue')\n",
    "plt.yticks(range(len(top_10)), [name[:30] + '...' if len(name) > 30 else name \n",
    "                                for name in top_10['Approach']])\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Top 10 Model Performance')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "complexity_acc = benchmark_df.groupby('Complexity')['Test_Accuracy'].mean()\n",
    "plt.bar(complexity_acc.index, complexity_acc.values, color=['green', 'orange', 'red'])\n",
    "plt.title('Accuracy by Model Complexity')\n",
    "plt.ylabel('Average Test Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(benchmark_df['Feature_Count'], benchmark_df['Test_Accuracy'], alpha=0.6)\n",
    "plt.xlabel('Feature Count')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Accuracy vs Feature Count')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Show improvement over baseline\n",
    "baseline_acc = min(benchmark_df['Test_Accuracy'])\n",
    "improvements = benchmark_df['Test_Accuracy'] - baseline_acc\n",
    "plt.hist(improvements, bins=20, color='lightgreen', alpha=0.7)\n",
    "plt.xlabel('Improvement over Baseline')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Model Improvements')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n HYBRID MODEL DEVELOPMENT COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Best Model Achieved:\")\n",
    "print(f\" Accuracy: {best_accuracy:.3f}\")\n",
    "print(f\" Features: {combination_names[best_combination]}\")\n",
    "print(f\" Algorithm: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\" Features Used: {train_combinations[best_combination].shape[1]}\")\n",
    "\n",
    "print(f\"\\n Files Ready for Member 5 (Streamlit App):\")\n",
    "print(\" final_hybrid_model.pkl\")\n",
    "print(\" hybrid_model.pkl\")\n",
    "print(\" hybrid_scaler.pkl\")\n",
    "print(\" hybrid_config.json\")\n",
    "print(\" deployment_config.json\")\n",
    "print(\" performance_benchmark.csv\")\n",
    "\n",
    "print(f\"\\nðŸ‘¥ Integration Notes for Member 5:\")\n",
    "print(\" Use hybrid_config.json to understand model requirements\")\n",
    "print(\" Load models with joblib.load()\")\n",
    "print(\" Use deployment_config.json for metadata encoding maps\")\n",
    "print(\" Test function 'predict_with_hybrid_model' is ready to use\")\n",
    "\n",
    "print(f\"\\n READY FOR STREAMLITÂ DEPLOYMENT!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
